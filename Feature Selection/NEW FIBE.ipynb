{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up dependency(s)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from numpy import hstack\n",
    "from numpy import asarray\n",
    "\n",
    "## plot method\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Image and HTML\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from IPython.display import display\n",
    "\n",
    "## Metices\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SKlearn Models\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## feature ranking\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import itertools\n",
    "\n",
    "## Boosting Method\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "## regex \n",
    "import re\n",
    "\n",
    "## resample technique\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## time \n",
    "import time\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45321d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df, target, flag):\n",
    "    \n",
    "    ### df: dataframe\n",
    "    ### target : target that need to be upsampling\n",
    "    ### flag: show details\n",
    "    ### return a dataframe with balanced target\n",
    "    \n",
    "    if flag:\n",
    "        plt.figure(figsize=(8,4)) \n",
    "        ax = sns.countplot(x= target , data=df)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "        ax.set_title('How many counts for categorical feature: {}'.format(target_))\n",
    "\n",
    "    value_dic = df[target].value_counts()\n",
    "    target_value = value_dic.keys()\n",
    "    \n",
    "    majority = target_value[0]\n",
    "    majority_value = value_dic[majority]\n",
    "    \n",
    "    df_majority = df[(df[target]==majority)] \n",
    "    \n",
    "    l = [df_majority]\n",
    "    \n",
    "    for minority in target_value[1:]:\n",
    "        #print('what is minority: ', minority)\n",
    "        df_minority = df[(df[target] == minority)] \n",
    "\n",
    "        df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,    # sample with replacement\n",
    "                                     n_samples= majority_value, # to match majority class\n",
    "                                     random_state=42)  # reproducible results\n",
    "        \n",
    "        l.append(df_minority_upsampled)\n",
    "         \n",
    "    \n",
    "    output = pd.concat(l)\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def flatten_list(x):\n",
    "    ### flatten a list\n",
    "    ### e.g. [1, [2,3], 4]\n",
    "    ### return [1,2,3,4]\n",
    "    \n",
    "    # input is a list of list\n",
    "    # output is a flattened list \n",
    "    \n",
    "    result = []\n",
    "    for i in x:\n",
    "        if isinstance(i,list):\n",
    "            result.extend(i)\n",
    "        else:\n",
    "            result.append(i)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def forward_include(clf, \n",
    "                    df_x, \n",
    "                    df_y,\n",
    "                    select_feature,\n",
    "                    all_features,\n",
    "                    score):\n",
    "    \n",
    "    ### clf: classifier\n",
    "    ### df_x, df_y : use to train the model\n",
    "    ### select_feature : features already included\n",
    "    ### all_features : all the features\n",
    "    ### score: best score (for classification : use ACC)\n",
    "    \n",
    "    ### return a tuple (selected feature, best_score)\n",
    "    \n",
    "    select_feature , score_predict = forward_onestep(clf, \n",
    "                                             df_x, df_y,\n",
    "                                             select_feature,all_features,\n",
    "                                             score)\n",
    "    \n",
    "    while score_predict >= score:\n",
    "        score = score_predict\n",
    "        select_feature , score_predict = forward_onestep(clf, \n",
    "                                             df_x, df_y,\n",
    "                                             select_feature,all_features,\n",
    "                                             score)\n",
    "        \n",
    "    return select_feature, score\n",
    "\n",
    "\n",
    "def forward_onestep(clf, \n",
    "                    df_x, \n",
    "                    df_y, \n",
    "                    select_feature, \n",
    "                    all_features,\n",
    "                    score):\n",
    "    \n",
    "    ### make one move forward inclusion, used in the function forward include\n",
    "    ### use sample with replacement, include the elements that maximize the score\n",
    "    \n",
    "    ### clf: classifier\n",
    "    ### df_x , df_y is the dataframe X, y that will be put int the model\n",
    "    ### select_feature : features already included\n",
    "    ### all_features : all the features\n",
    "    ### score: best score (for classification : use ACC)\n",
    "    \n",
    "    ### return a tuple (selected feature, best_score)\n",
    "    \n",
    "    \n",
    "    score_list = []\n",
    "    feature_list = []\n",
    "    \n",
    "    left_feature = [i for i in all_features if i not in select_feature]\n",
    "    \n",
    "    if len(left_feature) != 0:\n",
    "        for feature in left_feature:\n",
    "\n",
    "            feature_list.append(feature)\n",
    "\n",
    "            select_feature.append(feature)\n",
    "\n",
    "            feature_input = flatten_list(select_feature)\n",
    "\n",
    "            select_feature.pop(-1)\n",
    "\n",
    "            acc = np.mean(cross_val_score(clf, df_x[feature_input], df_y, cv=5))\n",
    "\n",
    "            score_list.append(acc)\n",
    "\n",
    "        highest_index = np.argmax(score_list)\n",
    "\n",
    "        if score_list[highest_index] >= score:\n",
    "            select_feature.append(feature_list[highest_index])\n",
    "            return select_feature, score_list[highest_index]\n",
    "\n",
    "        else:\n",
    "            return select_feature, -1\n",
    "    else:\n",
    "        return select_feature, -1\n",
    "    \n",
    "\n",
    "\n",
    "def forward_include2(clf, \n",
    "                     df_x, \n",
    "                     df_y,\n",
    "                     select_feature,\n",
    "                     all_features,\n",
    "                     score):\n",
    "    \n",
    "    ### clf: classifier\n",
    "    ### df_x, df_y : use to train the model\n",
    "    ### select_feature : features already included\n",
    "    ### all_features : all the features\n",
    "    ### score: best score (for classification : use ACC)\n",
    "    \n",
    "    ### return a tuple (selected feature, best_score)\n",
    "    \n",
    "    select_list = []\n",
    "    score_list = []\n",
    "    \n",
    "    left_features = [i for i in all_features if i not in select_feature]\n",
    "    \n",
    "    if left_features != []:\n",
    "        for i in left_features:\n",
    "\n",
    "            max_score = score\n",
    "\n",
    "            select_feature_copy = [i for i in select_feature]\n",
    "            select_feature_copy.append(i)\n",
    "\n",
    "            feature_input = flatten_list(select_feature_copy)\n",
    "            score_predict = np.mean(cross_val_score(clf, df_x[feature_input], df_y, cv=5))\n",
    "\n",
    "            while score_predict >= max_score:\n",
    "                max_score = score_predict\n",
    "                select_feature_copy , score_predict = forward_onestep(clf, \n",
    "                                                                 df_x, \n",
    "                                                                 df_y,\n",
    "                                                                 select_feature_copy,\n",
    "                                                                 all_features,\n",
    "                                                                 max_score)\n",
    "\n",
    "            select_list.append(select_feature_copy)\n",
    "            score_list.append(max_score)\n",
    "\n",
    "        max_index = np.argmax(score_list)\n",
    "        \n",
    "        print(select_list)\n",
    "        print(score_list)\n",
    "        return select_list[max_index], score_list[max_index]\n",
    "    \n",
    "    else:\n",
    "        return select_feature, score\n",
    "    \n",
    "\n",
    "\n",
    "def backward_elimination(clf,\n",
    "                         df_x, \n",
    "                         df_y,\n",
    "                         select_feature,\n",
    "                         score):\n",
    "    \n",
    "    ### df_x, df_y : use to train the model\n",
    "    ### select_feature : features already included\n",
    "    ### all_features : all the features\n",
    "    ### score: best score (for classification : use ACC)\n",
    "    \n",
    "    ### return a tuple (selected feature, best_score)\n",
    "    \n",
    "    r = len(select_feature) - 1\n",
    "    feature_list = []\n",
    "    Acc_list = []\n",
    "    \n",
    "    for i in itertools.combinations(select_feature,r):\n",
    "        feature_list.append(list(i))\n",
    "        \n",
    "    for feature in feature_list:\n",
    "        feature_input = flatten_list(feature)\n",
    "        acc = np.mean(cross_val_score(clf, df_x[feature_input], df_y, cv=5))\n",
    "        Acc_list.append(acc)\n",
    "        \n",
    "    max_index = np.argmax(Acc_list)\n",
    "    \n",
    "    if Acc_list[max_index] >= score:\n",
    "        return feature_list[max_index], Acc_list[max_index], True\n",
    "    else:\n",
    "        return select_feature, score, False\n",
    "    \n",
    "\n",
    "\n",
    "def stepwise(clf, df_x, df_y, all_features):\n",
    "    \n",
    "    ## stepwise FIBE \n",
    "    ## At each step of forward inclusion, we perform backward elimination\n",
    "    \n",
    "    ### clf: classifier\n",
    "    ### df_x, df_y : use to train the model\n",
    "    ### all_features : all the features\n",
    "    \n",
    "    ### return a tuple (selected feature, best_score)\n",
    "    \n",
    "    select_feature = [] \n",
    "    score = -np.inf\n",
    "    \n",
    "    end_flag = True\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        end_flag = True\n",
    "        select_feature , score = forward_include2(clf, df_x, df_y, select_feature,all_features, score)\n",
    "        print('select_feature after forward inclusion: ', select_feature)\n",
    "        print('current score is :', score)\n",
    "  \n",
    "        current_feature = [i for i in select_feature]\n",
    "        \n",
    "        while end_flag == True and len(select_feature) >= 2:\n",
    "            select_feature, score, end_flag  = backward_elimination(clf, df_x, df_y, select_feature, score)\n",
    "            print('select_feature after backward elimination: ', select_feature)\n",
    "            print('current score is :', score)\n",
    "            \n",
    "        if current_feature == select_feature:\n",
    "            break\n",
    "        else:\n",
    "            remove_feature = [i for i in current_feature if i not in select_feature]\n",
    "            all_features = [i for i in all_features if i not in remove_feature]\n",
    "            \n",
    "            \n",
    "    return select_feature, score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
