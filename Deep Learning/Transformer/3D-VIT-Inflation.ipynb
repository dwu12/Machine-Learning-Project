{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_conv2d_to_conv3d(conv2d, depth_dim=3):\n",
    "    # conv2d has weights of shape (out_channels, in_channels, H, W)\n",
    "    weights_2d = conv2d.weight.data\n",
    "    out_channels, in_channels, H, W = weights_2d.shape\n",
    "    \n",
    "    # Inflate the weights by adding the depth as the last dimension\n",
    "    # Since medical image has channel = 1, then we will need to change back to greyscale\n",
    "    # The weights are rearranged to shape (out_channels, in_channels, H, W, depth_dim)\n",
    "    # Then, normalize by dividing by depth_dim\n",
    "    weights_2d = weights_2d.sum(1, keepdim = True)\n",
    "    weights_3d = weights_2d.unsqueeze(-1).repeat(1, 1, 1, 1, depth_dim) / depth_dim\n",
    "    \n",
    "    # Create a new 3D convolutional layer\n",
    "    # Note the adjustment in the kernel and stride sizes to accommodate depth as the last dimension\n",
    "    conv3d = nn.Conv3d(in_channels, out_channels, (H, W, depth_dim), stride=(16, 16, depth_dim))\n",
    "    \n",
    "    # Set the weights from the 2D conv layer to the 3D conv layer\n",
    "    conv3d.weight.data = weights_3d\n",
    "    \n",
    "    return conv3d\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    # Add adaptater block\n",
    "    # go down and then go up \n",
    "    def __init__(self, D_features, mlp_ratio=0.25, act_layer=nn.GELU, skip_connect=True):\n",
    "        super().__init__()\n",
    "        self.skip_connect = skip_connect\n",
    "        D_hidden_features = int(D_features * mlp_ratio)\n",
    "        self.act = act_layer()\n",
    "        self.D_fc1 = nn.Linear(D_features, D_hidden_features)\n",
    "        self.D_fc2 = nn.Linear(D_hidden_features, D_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is (BT, HW+1, D)\n",
    "        xs = self.D_fc1(x)\n",
    "        xs = self.act(xs)\n",
    "        xs = self.D_fc2(xs)\n",
    "        if self.skip_connect:\n",
    "            x = x + xs\n",
    "        else:\n",
    "            x = xs\n",
    "        return x\n",
    "\n",
    "class AdapterEncoderLayer(nn.Module):\n",
    "    def __init__(self, original_layer, D_features):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.adapter = Adapter(D_features)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Apply original layer\n",
    "        output = self.original_layer(*args, **kwargs)\n",
    "        # Apply adapter\n",
    "        output = self.adapter(output[0])\n",
    "        return (output,)\n",
    "\n",
    "class ViT_inflated(nn.Module):\n",
    "    def __init__(self, num_class, depth_dim):\n",
    "        super().__init__()\n",
    "        model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "        ViT_embedding = model.vision_model\n",
    "\n",
    "        conv2d_layer = ViT_embedding.embeddings.patch_embedding\n",
    "        conv3d_layer = inflate_conv2d_to_conv3d(conv2d_layer, depth_dim)\n",
    "        ViT_embedding.embeddings.patch_embedding = conv3d_layer\n",
    "\n",
    "        for i in range(len(ViT_embedding.encoder.layers)):\n",
    "            ViT_embedding.encoder.layers[i] = AdapterEncoderLayer(ViT_embedding.encoder.layers[i], 768)\n",
    "        \n",
    "        self.model = ViT_embedding\n",
    "        self.proj_head = nn.Linear(in_features=768, out_features=num_class)\n",
    "\n",
    "        # Freeze all parameters in the model initially\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the parameters in the adapter layers and projection head\n",
    "        for layer in self.model.encoder.layers:\n",
    "            for param in layer.adapter.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        for param in self.proj_head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, image):\n",
    "        if not self.model: \n",
    "            raise Exception('ViT model does not load successfully')\n",
    "        \n",
    "        else:\n",
    "            print('Model Load successfully, inflated weight to 3D')\n",
    "        \n",
    "        output = self.model(image)[1]\n",
    "        output = self.proj_head(output)\n",
    "\n",
    "        return output \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diwu/opt/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Load successfully, inflated weight to 3D\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "model = ViT_inflated(2,150)\n",
    "image = torch.randn(1,1,224,224,150)\n",
    "print(model(image).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Load successfully, inflated weight to 3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type (var_name))                                                     Input Shape          Output Shape         Param #              Trainable\n",
       "===========================================================================================================================================================\n",
       "ViT_inflated (ViT_inflated)                                                 [1, 1, 224, 224, 150] [1, 2]               --                   Partial\n",
       "├─CLIPVisionTransformer (model)                                             [1, 1, 224, 224, 150] [1, 768]             --                   Partial\n",
       "│    └─CLIPVisionEmbeddings (embeddings)                                    [1, 1, 224, 224, 150] [1, 197, 768]        768                  False\n",
       "│    │    └─Conv3d (patch_embedding)                                        [1, 1, 224, 224, 150] [1, 768, 14, 14, 1]  (29,491,968)         False\n",
       "│    │    └─Embedding (position_embedding)                                  [1, 197]             [1, 197, 768]        (151,296)            False\n",
       "│    └─LayerNorm (pre_layrnorm)                                             [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "│    └─CLIPEncoder (encoder)                                                --                   [1, 197, 768]        --                   Partial\n",
       "│    │    └─ModuleList (layers)                                             --                   --                   88,604,928           Partial\n",
       "│    └─LayerNorm (post_layernorm)                                           [1, 768]             [1, 768]             (1,536)              False\n",
       "├─Linear (proj_head)                                                        [1, 768]             [1, 2]               1,538                True\n",
       "===========================================================================================================================================================\n",
       "Total params: 118,253,570\n",
       "Trainable params: 3,552,002\n",
       "Non-trainable params: 114,701,568\n",
       "Total mult-adds (G): 5.87\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 30.11\n",
       "Forward/backward pass size (MB): 181.56\n",
       "Params size (MB): 473.01\n",
       "Estimated Total Size (MB): 684.67\n",
       "==========================================================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model, \n",
    "        input_size=(1, 1, 224, 224,150), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
