{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUto6ZrnCpoiVtInB7NhCy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torch import nn\n","from torchvision import transforms"],"metadata":{"id":"vFmM0ZNr_OWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q torchinfo"],"metadata":{"id":"5kQaTorB_X57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchinfo import summary"],"metadata":{"id":"QiZZ7YT1_b6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WuYJaOFV_frU","executionInfo":{"status":"ok","timestamp":1665843207039,"user_tz":240,"elapsed":9,"user":{"displayName":"DI WU","userId":"00334041348669656891"}},"outputId":"9045a8bb-ab1c-4606-fbe2-a363d522798b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":[],"metadata":{"id":"47fR60LP_h4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Create a class which subclasses nn.Module\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, \n","                 in_channels:int=3,\n","                 patch_size:int=16,\n","                 embedding_dim:int=768):\n","        super().__init__()\n","        \n","        self.patch_size = patch_size\n","        \n","        # Create a layer to turn an image into patches\n","        self.patcher = nn.Conv2d(in_channels=in_channels,\n","                                 out_channels=embedding_dim,\n","                                 kernel_size=patch_size,\n","                                 stride=patch_size,\n","                                 padding=0)\n","\n","        # Create a layer to flatten the patch feature maps into a single dimension\n","        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n","                                  end_dim=3)\n","\n","    # Define the forward method \n","    def forward(self, x):\n","        # Create assertion to check that inputs are the correct shape\n","        image_resolution = x.shape[-1]\n","        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n","        \n","        # Perform the forward pass\n","        x_patched = self.patcher(x)\n","        x_flattened = self.flatten(x_patched) \n","        # Make sure the output shape has the right order \n","        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"],"metadata":{"id":"zS2GQSJT9wwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n","                                                       nhead=12,\n","                                                       dim_feedforward=3072,\n","                                                       dropout=0.1,\n","                                                       activation=\"gelu\",\n","                                                       batch_first=True,\n","                                                       norm_first=True)\n","transformer_encoder_layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__h5cdQT_x7i","executionInfo":{"status":"ok","timestamp":1665843276435,"user_tz":240,"elapsed":638,"user":{"displayName":"DI WU","userId":"00334041348669656891"}},"outputId":"747111d9-9de0-4315-8375-21e5a685e92f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TransformerEncoderLayer(\n","  (self_attn): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","  )\n","  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n","  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout1): Dropout(p=0.1, inplace=False)\n","  (dropout2): Dropout(p=0.1, inplace=False)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["transformer_encoder = nn.TransformerEncoder(\n","    encoder_layer=transformer_encoder_layer,\n","    num_layers=12)"],"metadata":{"id":"PFvA-BEy_yld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ViT(nn.Module): \n","  def __init__(self,\n","               img_size=224, # from Table 3\n","               num_channels=3,\n","               patch_size=16,\n","               embedding_dim=768, # from Table 1\n","               dropout=0.1, \n","               mlp_size=3072, # from Table 1\n","               num_transformer_layers=12, # from Table 1\n","               num_heads=12, # from Table 1 (number of multi-head self attention heads)\n","               num_classes=1000): # generic number of classes (this can be adjusted)\n","    super().__init__()\n","\n","    # Assert image size is divisible by patch size \n","    assert img_size % patch_size == 0, \"Image size must be divisble by patch size.\"\n","\n","    # Create patch embedding\n","    self.patch_embedding = PatchEmbedding(in_channels=num_channels,\n","                                          patch_size=patch_size,\n","                                          embedding_dim=embedding_dim)\n","\n","    # Create class token\n","    self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dim),\n","                                    requires_grad=True)\n","\n","    # Create positional embedding\n","    num_patches = (img_size * img_size) // patch_size**2 # N = HW/P^2\n","    self.positional_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim))\n","\n","    # Create patch + position embedding dropout \n","    self.embedding_dropout = nn.Dropout(p=dropout)\n","\n","    # Create Transformer Encoder layer (single)\n","    # self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n","    #                                                             nhead=num_heads,\n","    #                                                             dim_feedforward=mlp_size,\n","    #                                                             activation=\"gelu\",\n","    #                                                             batch_first=True,\n","    #                                                             norm_first=True)\n","\n","    # Create stack Transformer Encoder layers (stacked single layers)\n","    self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim,\n","                                                                                              nhead=num_heads,\n","                                                                                              dim_feedforward=mlp_size,\n","                                                                                              activation=\"gelu\",\n","                                                                                              batch_first=True,\n","                                                                                              norm_first=True), # Create a single Transformer Encoder Layer\n","                                                     num_layers=num_transformer_layers) # Stack it N times\n","\n","    # Create MLP head\n","    self.mlp_head = nn.Sequential(\n","        nn.LayerNorm(normalized_shape=embedding_dim),\n","        nn.Linear(in_features=embedding_dim,\n","                  out_features=num_classes)\n","    )\n","\n","  def forward(self, x):\n","    # Get some dimensions from x\n","    batch_size = x.shape[0]\n","\n","    # Create the patch embedding\n","    x = self.patch_embedding(x)\n","    # print(x.shape)\n","\n","    # First, expand the class token across the batch size\n","    class_token = self.class_token.expand(batch_size, -1, -1) # \"-1\" means infer the dimension\n","\n","    # Prepend the class token to the patch embedding\n","    x = torch.cat((class_token, x), dim=1)\n","    # print(x.shape)\n","\n","    # Add the positional embedding to patch embedding with class token\n","    x = self.positional_embedding + x\n","    # print(x.shape)\n","\n","    # Dropout on patch + positional embedding\n","    x = self.embedding_dropout(x)\n","\n","    # Pass embedding through Transformer Encoder stack\n","    x = self.transformer_encoder(x)\n","\n","    # Pass 0th index of x through MLP head\n","    x = self.mlp_head(x[:, 0])\n","\n","    return x"],"metadata":{"id":"FYPyKDh7_9uN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Maybe compare to pytorch pretrained model or huggingface"],"metadata":{"id":"O_yvp3uzHuks"}},{"cell_type":"code","source":["import torchvision\n","\n","# Download pretrained ViT weights and model\n","vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT\" means best available\n","pretrained_vit = torchvision.models.vit_b_16(weights=vit_weights)\n","\n","# Freeze all layers in pretrained ViT model \n","for param in pretrained_vit.parameters():\n","  param.requires_grad = False\n","\n","# Update the preatrained ViT head \n","embedding_dim = 768 # ViT_Base\n","class_names = [1,2,3,4]\n","pretrained_vit.heads = nn.Sequential(\n","    nn.LayerNorm(normalized_shape=embedding_dim),\n","    nn.Linear(in_features=embedding_dim, \n","              out_features=len(class_names))\n",")\n","\n","# Print a summary\n","summary(model=pretrained_vit, \n","        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fW33XhHHt6B","executionInfo":{"status":"ok","timestamp":1665845651106,"user_tz":240,"elapsed":1730,"user":{"displayName":"DI WU","userId":"00334041348669656891"}},"outputId":"afa8fb98-a122-435c-f74a-e04a7e5b0811"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 4]               768                  Partial\n","├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n","├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n","│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n","│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n","├─Sequential (heads)                                         [1, 768]             [1, 4]               --                   True\n","│    └─LayerNorm (0)                                         [1, 768]             [1, 768]             1,536                True\n","│    └─Linear (1)                                            [1, 768]             [1, 4]               3,076                True\n","============================================================================================================================================\n","Total params: 85,803,268\n","Trainable params: 4,612\n","Non-trainable params: 85,798,656\n","Total mult-adds (M): 172.47\n","============================================================================================================================================\n","Input size (MB): 0.60\n","Forward/backward pass size (MB): 104.09\n","Params size (MB): 229.21\n","Estimated Total Size (MB): 333.91\n","============================================================================================================================================"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":[],"metadata":{"id":"UpYmCv2xKBEo"}}]}