{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za86vmBb0bVT"
      },
      "outputs": [],
      "source": [
        "# Generative A.I course by Javier Ideami\n",
        "# Multimodal A.I CLIP+vqgan Notebook\n",
        "\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "#https://github.com/openai/CLIP \n",
        "#CLIP (Contrastive Language-Image Pre-Training) \n",
        "#Learning Transferable Visual Models From Natural Language Supervision\n",
        "#Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\n",
        "#Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n",
        "\n",
        "!git clone https://github.com/CompVis/taming-transformers  \n",
        "#https://github.com/CompVis/taming-transformers\n",
        "#Taming Transformers for High-Resolution Image Synthesis\n",
        "#Patrick Esser, Robin Rombach, Bj√∂rn Ommer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ompo3S_w7Ej6"
      },
      "outputs": [],
      "source": [
        "## install some extra libraries\n",
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "!pip uninstall torchtext --yes\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn_gTOEw7h8x"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import torch, os, imageio, pdb, math\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yaml \n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZehdKop7_bK"
      },
      "outputs": [],
      "source": [
        "## helper functions\n",
        "\n",
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transpose((1,2,0))\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def norm_data(data):\n",
        "  return (data.clip(-1,1)+1)/2 ### range between 0 and 1 in the result\n",
        "\n",
        "### Parameters \n",
        "learning_rate = .5\n",
        "batch_size = 1\n",
        "wd = .1 \n",
        "noise_factor = .22 \n",
        "\n",
        "total_iter=400\n",
        "im_shape = [450, 450, 3] # height, width, channel\n",
        "size1, size2, channels = im_shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfApIAoR-N55"
      },
      "outputs": [],
      "source": [
        "### CLIP MODEL ### \n",
        "clipmodel, _ = clip.load('ViT-B/32', jit=False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print(\"Clip model visual input resolution: \", clipmodel.visual.input_resolution)\n",
        "\n",
        "device=torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_MJ3iNdAW7Z"
      },
      "outputs": [],
      "source": [
        "## Taming transformer instantiation\n",
        "\n",
        "%cd taming-transformers/\n",
        "\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/configs\n",
        "\n",
        "if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt' \n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml' \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftDHxwo_B2F_"
      },
      "outputs": [],
      "source": [
        "from taming.models.vqgan import VQModel\n",
        "\n",
        "def load_config(config_path, display=False):\n",
        "   config_data = OmegaConf.load(config_path)\n",
        "   if display:\n",
        "     print(yaml.dump(OmegaConf.to_container(config_data)))\n",
        "   return config_data\n",
        "\n",
        "def load_vqgan(config, chk_path=None):\n",
        "  model = VQModel(**config.model.params)\n",
        "  if chk_path is not None:\n",
        "    state_dict = torch.load(chk_path, map_location=\"cpu\")[\"state_dict\"]\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "  return model.eval()\n",
        "\n",
        "def generator(x):\n",
        "  x = taming_model.post_quant_conv(x)\n",
        "  x = taming_model.decoder(x)\n",
        "  return x\n",
        "\n",
        "taming_config = load_config(\"./models/vqgan_imagenet_f16_16384/configs/model.yaml\", display=True)\n",
        "taming_model = load_vqgan(taming_config, chk_path=\"./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY6dOXE7Drgn"
      },
      "outputs": [],
      "source": [
        "### Declare the values that we are going to optimize\n",
        "\n",
        "class Parameters(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Parameters, self).__init__()\n",
        "    self.data = .5*torch.randn(batch_size, 256, size1//16, size2//16).cuda() # 1x256x14x25 (225/16, 400/16)\n",
        "    self.data = torch.nn.Parameter(torch.sin(self.data))\n",
        "\n",
        "  def forward(self):\n",
        "    return self.data\n",
        "\n",
        "def init_params():\n",
        "  params=Parameters().cuda()\n",
        "  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=wd)\n",
        "  return params, optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsMOgdHPGFI8"
      },
      "outputs": [],
      "source": [
        "### Encoding prompts and a few more things\n",
        "normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "def encodeText(text):\n",
        "  t=clip.tokenize(text).cuda()\n",
        "  t=clipmodel.encode_text(t).detach().clone()\n",
        "  return t\n",
        "\n",
        "def createEncodings(include, exclude, extras):\n",
        "  include_enc=[]\n",
        "  for text in include:\n",
        "    include_enc.append(encodeText(text))\n",
        "  exclude_enc=encodeText(exclude) if exclude != '' else 0\n",
        "  extras_enc=encodeText(extras) if extras !='' else 0\n",
        "\n",
        "  return include_enc, exclude_enc, extras_enc\n",
        "\n",
        "augTransform = torch.nn.Sequential(\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)  \n",
        ").cuda()\n",
        "\n",
        "Params, optimizer = init_params()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(Params().shape)\n",
        "  img= norm_data(generator(Params()).cpu()) # 1 x 3 x 224 x 400 [225 x 400]\n",
        "  print(\"img dimensions: \",img.shape)\n",
        "  show_from_tensor(img[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsgmO1LeI_5e"
      },
      "outputs": [],
      "source": [
        "### create crops\n",
        "\n",
        "def create_crops(img, num_crops=32): \n",
        "  p=size1//2\n",
        "  img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0) # 1 x 3 x 448 x 624 (adding 112*2 on all sides to 224x400)\n",
        "\n",
        "  img = augTransform(img) #RandomHorizontalFlip and RandomAffine\n",
        "\n",
        "  crop_set = []\n",
        "  for ch in range(num_crops):\n",
        "    gap1= int(torch.normal(1.2, .3, ()).clip(.43, 1.9) * size1)\n",
        "    offsetx = torch.randint(0, int(size1*2-gap1),())\n",
        "    offsety = torch.randint(0, int(size1*2-gap1),())\n",
        "\n",
        "    crop=img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]\n",
        "\n",
        "    crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)\n",
        "    crop_set.append(crop)\n",
        "\n",
        "  img_crops=torch.cat(crop_set,0) ## 32 x 3 x 224 x 224\n",
        "\n",
        "  randnormal = torch.randn_like(img_crops, requires_grad=False)\n",
        "  num_rands=0\n",
        "  randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda() #32\n",
        "  \n",
        "  for ns in range(num_rands):\n",
        "    randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
        "\n",
        "  img_crops = img_crops + noise_factor*randstotal*randnormal\n",
        "\n",
        "  return img_crops\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwESrZefM0Vt"
      },
      "outputs": [],
      "source": [
        "### Show current state of generation\n",
        "\n",
        "def showme(Params, show_crop):\n",
        "  with torch.no_grad():\n",
        "    generated = generator(Params())\n",
        "\n",
        "    if (show_crop):\n",
        "      print(\"Augmented cropped example\")\n",
        "      aug_gen = generated.float() # 1 x 3 x 224 x 400 \n",
        "      aug_gen = create_crops(aug_gen, num_crops=1)\n",
        "      aug_gen_norm = norm_data(aug_gen[0])\n",
        "      show_from_tensor(aug_gen_norm)\n",
        "\n",
        "    print(\"Generation\")\n",
        "    latest_gen=norm_data(generated.cpu()) # 1 x 3 x 224 x 400\n",
        "    show_from_tensor(latest_gen[0])\n",
        "\n",
        "  return (latest_gen[0]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uV4VJn7N-FI"
      },
      "outputs": [],
      "source": [
        "# Optimization process\n",
        "\n",
        "def optimize_result(Params, prompt):\n",
        "  alpha=1 ## the importance of the include encodings\n",
        "  beta=.5 ## the importance of the exclude encodings\n",
        "\n",
        "  ## image encoding\n",
        "  out = generator(Params())\n",
        "  out = norm_data(out)\n",
        "  out = create_crops(out)\n",
        "  out = normalize(out) # 30 x 3 x 224 x 224\n",
        "  image_enc=clipmodel.encode_image(out) ## 30 x 512\n",
        "\n",
        "  ## text encoding  w1 and w2\n",
        "  final_enc = w1*prompt + w2*extras_enc # prompt and extras_enc : 1 x 512\n",
        "  final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True) # 1 x 512\n",
        "  final_text_exclude_enc = exclude_enc\n",
        "\n",
        "  ## calculate the loss\n",
        "  main_loss = torch.cosine_similarity(final_text_include_enc, image_enc, -1) # 30\n",
        "  penalize_loss = torch.cosine_similarity(final_text_exclude_enc, image_enc, -1) # 30\n",
        "\n",
        "  final_loss = -alpha*main_loss + beta*penalize_loss\n",
        "\n",
        "  return final_loss\n",
        "\n",
        "def optimize(Params, optimizer, prompt):\n",
        "  loss = optimize_result(Params, prompt).mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vpVuN8iQuLb"
      },
      "outputs": [],
      "source": [
        "### training loop\n",
        "\n",
        "def training_loop(Params, optimizer, show_crop=False):\n",
        "  res_img=[]\n",
        "  res_z=[]\n",
        "\n",
        "  for prompt in include_enc:\n",
        "    iteration=0\n",
        "    Params, optimizer = init_params() # 1 x 256 x 14 x 25 (225/16, 400/16)\n",
        "\n",
        "    for it in range(total_iter):\n",
        "      loss = optimize(Params, optimizer, prompt)\n",
        "\n",
        "      if iteration>=80 and iteration%show_step == 0:\n",
        "        new_img = showme(Params, show_crop)\n",
        "        res_img.append(new_img)\n",
        "        res_z.append(Params()) # 1 x 256 x 14 x 25\n",
        "        print(\"loss:\", loss.item(), \"\\niteration:\",iteration)\n",
        "\n",
        "      iteration+=1\n",
        "    torch.cuda.empty_cache()\n",
        "  return res_img, res_z\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q4kwgudSuI5"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "#include=['sketch of a lady', 'sketch of a man on a horse']\n",
        "include=['A painting of a pineapple in a bowl']\n",
        "exclude='watermark'\n",
        "extras = \"\"\n",
        "w1=1\n",
        "w2=1\n",
        "noise_factor= .22\n",
        "total_iter=110\n",
        "show_step=10 # set this to see the result every 10 interations beyond iteration 80\n",
        "include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
        "res_img, res_z=training_loop(Params, optimizer, show_crop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScpytmgQUffY"
      },
      "outputs": [],
      "source": [
        "print(len(res_img), len(res_z))\n",
        "print(res_img[0].shape, res_z[0].shape)\n",
        "print(res_z[0].max(), res_z[0].min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_NnBA9AXaqg"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "include=['A forest with purple trees', 'an elephant at the top of a mountain, looking at the stars','one hundred people with green jackets']\n",
        "exclude='watermark, cropped, confusing, incoherent, cut, blurry'\n",
        "extras = \"\"\n",
        "w1=1\n",
        "w2=1\n",
        "noise_factor= .22\n",
        "total_iter=110\n",
        "show_step=total_iter-1 # set this if you want to interpolate between only the final versions\n",
        "include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
        "res_img, res_z=training_loop(Params, optimizer, show_crop=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3lTWZaDYBJR"
      },
      "outputs": [],
      "source": [
        "def interpolate(res_z_list, duration_list):\n",
        "  gen_img_list=[]\n",
        "  fps = 25\n",
        "\n",
        "  for idx, (z, duration) in enumerate(zip(res_z_list, duration_list)):\n",
        "    num_steps = int(duration*fps)\n",
        "    z1=z\n",
        "    z2=res_z_list[(idx+1)%len(res_z_list)] # 1 x 256 x 14 x 25 (225/16, 400/16)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "      alpha = math.sin(1.5*step/num_steps)**6\n",
        "      z_new = alpha * z2 + (1-alpha) * z1\n",
        "\n",
        "      new_gen=norm_data(generator(z_new).cpu())[0] ## 3 x 224 x 400\n",
        "      new_img=T.ToPILImage(mode='RGB')(new_gen)\n",
        "      gen_img_list.append(new_img)\n",
        "\n",
        "  return gen_img_list\n",
        "\n",
        "durations=[5,5,5,5,5,5]\n",
        "interp_result_img_list = interpolate(res_z, durations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeSmO7WRasl_"
      },
      "outputs": [],
      "source": [
        "## create a video\n",
        "out_video_path=f\"../video.mp4\"\n",
        "writer = imageio.get_writer(out_video_path, fps=25)\n",
        "for pil_img in interp_result_img_list:\n",
        "  img = np.array(pil_img, dtype=np.uint8)\n",
        "  writer.append_data(img)\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmlxcyLzbbu_"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('../video.mp4','rb').read()\n",
        "data=\"data:video/mp4;base64,\"+b64encode(mp4).decode()\n",
        "HTML(\"\"\"<video width=800 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g77m5Urb8YH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Multimodal Generation - Generative A.I course by Ideami",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
