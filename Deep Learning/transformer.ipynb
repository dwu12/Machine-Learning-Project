{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout = 0.2):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None, dropout = None):\n",
    "        # query: [batchsize, query_len, dim]\n",
    "        # key  : [batchsize, key_len  , dim]\n",
    "        # value: [batchsize, value_len, dim]\n",
    "\n",
    "        dim = query.shape[-1]\n",
    "\n",
    "        score = torch.bmm(query, key.transpose(-2,-1)) / np.sqrt(dim) # [batchsize, query_len, key_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            socre = score.mask_fill(mask == 0, 1e-9)\n",
    "\n",
    "        attention_weight = self.softmax(score) \n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        return torch.bmm(attention_weight, value) # [batchsize, query_len, dim]\n",
    "\n",
    "\n",
    "\n",
    "query = torch.randn(32, 10, 256)\n",
    "key = torch.randn(32, 8, 256)\n",
    "value = torch.randn(32, 8, 256)\n",
    "Attention = ScaledDotProductAttention()\n",
    "\n",
    "assert Attention(query, key, value, None, None).shape == (32, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, hidden_size, dropout = 0.2 , bias = False):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        self.query  = nn.Linear(query_size, hidden_size, bias = bias)\n",
    "        self.key    = nn.Linear(key_size, hidden_size, bias = bias)\n",
    "        self.value  = nn.Linear(hidden_size, 1, bias = bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, query, key, value, mask= None, dropout = None):\n",
    "        # query : [batchsize, query_len, query_size]\n",
    "        # key   : [batchsize, key_len ,  key_size]\n",
    "        # value : [batchsize, value_len, value_size]\n",
    "\n",
    "        querys = self.query(query).unsqueeze(2) #[batchsize, query_len, 1, hidden_size]\n",
    "        keys   = self.key(key).unsqueeze(1)     #[batchsize, 1, key_len,  hidden_size]\n",
    "        output = torch.tanh(querys + keys)   #[batchsize, query_len, key_len, num_hiddens]\n",
    "\n",
    "        score = self.value(output).squeeze(-1) #[batchsize, query_len, key_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, 1e-9)\n",
    "\n",
    "        weight = self.softmax(score) #[batchsize, query_len, key_len] \n",
    "\n",
    "        if dropout is not None:\n",
    "            weight = self.dropout(weight)\n",
    "\n",
    "        return torch.bmm(weight, value) #[batchsize, query_len, key_len]\n",
    "        \n",
    "query = torch.randn(32, 10, 256)\n",
    "key = torch.randn(32, 8, 256)\n",
    "value = torch.randn(32, 8, 256)\n",
    "Attention = AdditiveAttention(256, 256, 512)\n",
    "\n",
    "assert Attention(query, key, value, None, None).shape == (32, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,  \n",
    "                 query_size,\n",
    "                 key_size,  \n",
    "                 value_size, \n",
    "                 hidden_size, \n",
    "                 num_heads, \n",
    "                 dropout = 0.2,\n",
    "                 bias=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads \n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "\n",
    "        self.query  = nn.Linear(query_size, hidden_size, bias = bias)\n",
    "        self.key    = nn.Linear(key_size, hidden_size, bias = bias)\n",
    "        self.value  = nn.Linear(value_size, hidden_size, bias = bias)\n",
    "        self.output = nn.Linear(hidden_size, hidden_size, bias= bias )\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        # query : [batchsize, query_len,  query_size]\n",
    "        # key   : [batchsize, key_len  ,  key_size]\n",
    "        # value : [batchsize, value_len , value_size]\n",
    "\n",
    "        querys = self.query(query) #[batchsize, query_len,  hidden_size]\n",
    "        keys   = self.key(key)     #[batchsize, key_len,  hidden_size]\n",
    "        values = self.value(value) #[batchsize, value_len,  hidden_size]\n",
    "\n",
    "        querys = querys.reshape(querys.shape[0], querys.shape[1], self.num_heads, self.head_dim) #[batchsize, query_len, num_heads, head_dim]\n",
    "        querys = querys.permute(0,2,1,3) #[batchsize, num_heads, query_len, head_dim]\n",
    "        querys = querys.reshape(-1, querys.shape[2], querys.shape[3]) #[batchsize * num_heads, query_len, head_dim]\n",
    "\n",
    "        keys   = keys.reshape(keys.shape[0], keys.shape[1], self.num_heads, self.head_dim) #[batchsize, key_len, num_heads, head_dim]\n",
    "        keys   = keys.permute(0,2,1,3) #[batchsize, num_heads, key_len, head_dim]\n",
    "        keys   = keys.reshape(-1, keys.shape[2], keys.shape[3]) #[batchsize * num_heads, key_len, head_dim]\n",
    "\n",
    "        values = values.reshape(values.shape[0], values.shape[1], self.num_heads, self.head_dim) #[batchsize, value_len, num_heads, head_dim]\n",
    "        values = values.permute(0,2,1,3) #[batchsize, num_heads, value_len, head_dim]\n",
    "        values = values.reshape(-1, values.shape[2], values.shape[3]) #[batchsize * num_heads, value_len, head_dim]\n",
    "\n",
    "        output = self.attention(querys, keys, values, mask) # [batchsize * num_heads, query_len, head_dim]\n",
    "        output = output.reshape(-1, self.num_heads, output.shape[1], output.shape[2]) #[batchsize, num_heads, query_len, head_dim]\n",
    "        output = output.permute(0,2,1,3) #[batchsize, query_len, num_heads, head_dim]\n",
    "        output = output.reshape(output.shape[0], output.shape[1], -1) #[batchsize, query_len, hidden_size]\n",
    "\n",
    "        output = self.output(output) #[batchsize, query_len, hidden_size]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "query = torch.randn(2,100,512)\n",
    "key = torch.randn(2,100,512)\n",
    "value = torch.randn(2,100,512)\n",
    "attention = MultiHeadAttention(512, 512, 512, 512, 8)\n",
    "\n",
    "assert attention(query, key, value, None).shape == (2,100,512)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, hidden_size, ff_dim, dropout = 0.2):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, ff_dim)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(ff_dim, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(2, 100, 512)\n",
    "model = PositionWiseFFN(512, 2048)\n",
    "assert model(x).shape == (2,100,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, size, dropout = 0.2):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self, x, layer_output):\n",
    "        return self.ln(self.dropout(layer_output) + x)\n",
    "    \n",
    "addnorm = AddNorm(512, 0.1)\n",
    "layer_output = torch.randn(2, 100, 512)\n",
    "x = torch.randn(2, 100, 512)\n",
    "\n",
    "assert addnorm(x, layer_output).shape == (2, 100, 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 query_size, \n",
    "                 key_size, \n",
    "                 value_size, \n",
    "                 hidden_size, \n",
    "                 ff_dim,\n",
    "                 num_heads, \n",
    "                 dropout = 0.2, \n",
    "                 use_bias = False):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(query_size, key_size, value_size, hidden_size, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(hidden_size, dropout)\n",
    "        self.ffn = PositionWiseFFN(hidden_size, ff_dim)\n",
    "        self.addnorm2 = AddNorm(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        x1 = self.addnorm1(query , self.attention(query, key, value, mask))\n",
    "        return self.addnorm2(x1, self.ffn(x1))\n",
    "    \n",
    "\n",
    "model = TransformerBlock(512, 512, 512, 512, 2048, 8)\n",
    "x = torch.randn((2,100,512))\n",
    "assert model(x, x, x, None).shape == (2, 100, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 hidden_size, \n",
    "                 ff_dim, \n",
    "                 num_heads, \n",
    "                 num_layers, \n",
    "                 dropout = 0.2, \n",
    "                 use_bias = False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blocks.add_module(\"block\" + str(i), \n",
    "                                   TransformerBlock(hidden_size, hidden_size, hidden_size, hidden_size, ff_dim, num_heads, dropout, use_bias))\n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        x = self.pos_encoding(self.embedding(x) * np.sqrt(self.hidden_size))\n",
    "        for block in self.blocks:\n",
    "            x = block(x, x, x, mask)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "encoder = Encoder(1024, 512, 2048, 8 , 8)\n",
    "x = torch.ones((2,100), dtype = torch.long)\n",
    "assert encoder(x, None).shape == (2, 100, 512)               "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 query_size, \n",
    "                 key_size, \n",
    "                 value_size, \n",
    "                 hidden_size,\n",
    "                 ff_dim, \n",
    "                 num_heads,\n",
    "                 dropout  = 0.1,\n",
    "                 use_bias = False):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(query_size, key_size, value_size, hidden_size, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(hidden_size, dropout)\n",
    "        self.TransformerBlock = TransformerBlock(query_size, key_size, value_size, hidden_size, ff_dim, num_heads, dropout, use_bias)\n",
    "\n",
    "    def forward(self, query, key, value, trg_mask, src_mask):\n",
    "        x = self.attention(query, query, query, trg_mask)\n",
    "        x = self.addnorm1(query, x)\n",
    "        x = self.TransformerBlock(x, key, value, src_mask)\n",
    "        return x \n",
    "    \n",
    "model = DecoderBlock(512, 512, 512, 512, 2048, 8)\n",
    "x = torch.randn((2,100,512))\n",
    "assert model(x, x, x, None, None).shape == (2,100,512)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 hidden_size,\n",
    "                 ff_dim, \n",
    "                 num_heads, \n",
    "                 num_layers, \n",
    "                 dropout = 0.2, \n",
    "                 use_bias = False):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n",
    "        self.generator  = Generator(hidden_size, vocab_size)\n",
    "\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blocks.add_module(\"block\" + str(i), \n",
    "                                   DecoderBlock(hidden_size, hidden_size, hidden_size, hidden_size, ff_dim, num_heads, dropout , use_bias))\n",
    "            \n",
    "    def forward(self, x , enc_out, trg_mask, src_mask):\n",
    "        x = self.pos_encoding(self.embedding(x) * np.sqrt(self.hidden_size))\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_out, enc_out, trg_mask, src_mask)\n",
    "\n",
    "        return self.generator(x)\n",
    "    \n",
    "decoder = Decoder(1024, 512, 2048, 8 , 8)\n",
    "x = torch.ones((2,100), dtype = torch.long)\n",
    "enc_out = torch.randn(2,100,512)\n",
    "assert decoder(x, enc_out,  None, None).shape == (2, 100, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
