{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8622,"status":"ok","timestamp":1686368558521,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"5otL3isKZZh_","outputId":"0e6fbd43-91ad-44ef-bca6-cfd74e3d678d"},"outputs":[{"data":{"text/plain":["\u003ctorch._C.Generator at 0x7fdd3c64cef0\u003e"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from torchvision.utils import make_grid\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":161,"status":"ok","timestamp":1686368567881,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"dg3zQ3MhZePY"},"outputs":[],"source":["def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n","    '''\n","    Function for visualizing images: Given a tensor of images, number of images, and\n","    size per image, plots and prints the images in an uniform grid.\n","    '''\n","    image_tensor = (image_tensor + 1) / 2\n","    image_unflat = image_tensor.detach().cpu()\n","    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n","    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n","    if show:\n","        plt.show()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":153,"status":"ok","timestamp":1686368571527,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"0Es3nl0zZgAa"},"outputs":[],"source":["class Generator(nn.Module):\n","    '''\n","    Generator Class\n","    Values:\n","        input_dim: the dimension of the input vector, a scalar\n","        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n","              (MNIST is black-and-white, so 1 channel is your default)\n","        hidden_dim: the inner dimension, a scalar\n","    '''\n","    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n","        super(Generator, self).__init__()\n","        self.input_dim = input_dim\n","        # Build the neural network\n","        self.gen = nn.Sequential(\n","            self.make_gen_block(input_dim, hidden_dim * 4),\n","            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n","            self.make_gen_block(hidden_dim * 2, hidden_dim),\n","            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n","        )\n","\n","    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n","        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n","        Parameters:\n","            input_channels: how many channels the input feature representation has\n","            output_channels: how many channels the output feature representation should have\n","            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","            stride: the stride of the convolution\n","            final_layer: a boolean, true if it is the final layer and false otherwise \n","                      (affects activation and batchnorm)\n","        '''\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.ReLU(inplace=True),\n","            )\n","        else:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.Tanh(),\n","            )\n","\n","    def forward(self, noise):\n","        '''\n","        Function for completing a forward pass of the generator: Given a noise tensor, \n","        returns generated images.\n","        Parameters:\n","            noise: a noise tensor with dimensions (n_samples, input_dim)\n","        '''\n","        x = noise.view(len(noise), self.input_dim, 1, 1)\n","        return self.gen(x)\n","\n","def get_noise(n_samples, input_dim, device='cpu'):\n","    '''\n","    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n","    creates a tensor of that shape filled with random numbers from the normal distribution.\n","    Parameters:\n","        n_samples: the number of samples to generate, a scalar\n","        input_dim: the dimension of the input vector, a scalar\n","        device: the device type\n","    '''\n","    return torch.randn(n_samples, input_dim, device=device)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686368583404,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"10cXZwGdZlXW"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    '''\n","    Discriminator Class\n","    Values:\n","      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n","            (MNIST is black-and-white, so 1 channel is your default)\n","      hidden_dim: the inner dimension, a scalar\n","    '''\n","    def __init__(self, im_chan=1, hidden_dim=64):\n","        super(Discriminator, self).__init__()\n","        self.disc = nn.Sequential(\n","            self.make_disc_block(im_chan, hidden_dim),\n","            self.make_disc_block(hidden_dim, hidden_dim * 2),\n","            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n","        )\n","\n","    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n","        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n","        Parameters:\n","            input_channels: how many channels the input feature representation has\n","            output_channels: how many channels the output feature representation should have\n","            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","            stride: the stride of the convolution\n","            final_layer: a boolean, true if it is the final layer and false otherwise \n","                      (affects activation and batchnorm)\n","        '''\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.LeakyReLU(0.2, inplace=True),\n","            )\n","        else:\n","            return nn.Sequential(\n","                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n","            )\n","\n","    def forward(self, image):\n","        '''\n","        Function for completing a forward pass of the discriminator: Given an image tensor, \n","        returns a 1-dimension tensor representing fake/real.\n","        Parameters:\n","            image: a flattened image tensor with dimension (im_chan)\n","        '''\n","        disc_pred = self.disc(image)\n","        return disc_pred.view(len(disc_pred), -1)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":153,"status":"ok","timestamp":1686368617038,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"PUUSaLgAZoFE"},"outputs":[],"source":["import torch.nn.functional as F\n","def get_one_hot_labels(labels, n_classes):\n","    '''\n","    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n","    Parameters:\n","        labels: tensor of labels from the dataloader, size (?)\n","        n_classes: the total number of classes in the dataset, an integer scalar\n","    '''\n","    return F.one_hot(labels, num_classes= n_classes)\n","\n","def combine_vectors(x, y):\n","    '''\n","    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n","    Parameters:\n","      x: (n_samples, ?) the first vector. \n","        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n","        but you shouldn't need to know the second dimension's size.\n","      y: (n_samples, ?) the second vector.\n","        Once again, in this assignment this will be the one-hot class vector \n","        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n","    '''\n","    # Note: Make sure this function outputs a float no matter what inputs it receives\n","    combined = torch.cat((x.float(),y.float()),dim = 1)\n","    return combined\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1407,"status":"ok","timestamp":1686368644165,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"qVPI_MGAZwXC","outputId":"83e93a6f-dc08-4c7e-c8f0-34d6f6bd74ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00\u003c00:00, 104897252.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00\u003c00:00, 40786428.90it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 1648877/1648877 [00:00\u003c00:00, 26065539.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00\u003c00:00, 9506251.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]}],"source":["mnist_shape = (1, 28, 28)\n","n_classes = 10\n","\n","criterion = nn.BCEWithLogitsLoss()\n","n_epochs = 200\n","z_dim = 64\n","display_step = 500\n","batch_size = 128\n","lr = 0.0002\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","dataloader = DataLoader(\n","    MNIST('.', download=True, transform=transform),\n","    batch_size=batch_size,\n","    shuffle=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":144,"status":"ok","timestamp":1686368654101,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"tU493pTRZ205"},"outputs":[],"source":["def get_input_dimensions(z_dim, mnist_shape, n_classes):\n","    '''\n","    Function for getting the size of the conditional input dimensions \n","    from z_dim, the image shape, and number of classes.\n","    Parameters:\n","        z_dim: the dimension of the noise vector, a scalar\n","        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)\n","        n_classes: the total number of classes in the dataset, an integer scalar\n","                (10 for MNIST)\n","    Returns: \n","        generator_input_dim: the input dimensionality of the conditional generator, \n","                          which takes the noise and class vectors\n","        discriminator_im_chan: the number of input channels to the discriminator\n","                            (e.g. C x 28 x 28 for MNIST)\n","    '''\n","    generator_input_dim = z_dim + n_classes\n","    discriminator_im_chan = mnist_shape[0] + n_classes\n","    return generator_input_dim, discriminator_im_chan"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1686368670456,"user":{"displayName":"DI WU","userId":"00334041348669656891"},"user_tz":240},"id":"UA8U5KdFZ5j0"},"outputs":[],"source":["generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n","\n","gen = Generator(input_dim=generator_input_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n","disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n","disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)\n","gen = gen.apply(weights_init)\n","disc = disc.apply(weights_init)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":87},"id":"7St-gu99Z93F"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3916340b8be64bdebf6e3c56c5915772","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/469 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\n"]}],"source":["cur_step = 0\n","generator_losses = []\n","discriminator_losses = []\n","\n","#UNIT TEST NOTE: Initializations needed for grading\n","noise_and_labels = False\n","fake = False\n","\n","fake_image_and_labels = False\n","real_image_and_labels = False\n","disc_fake_pred = False\n","disc_real_pred = False\n","\n","for epoch in range(n_epochs):\n","    # Dataloader returns the batches and the labels\n","    for real, labels in tqdm(dataloader):\n","        cur_batch_size = len(real)\n","        # Flatten the batch of real images from the dataset\n","        real = real.to(device)\n","\n","        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n","        image_one_hot_labels = one_hot_labels[:, :, None, None]\n","        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n","\n","        ### Update discriminator ###\n","        # Zero out the discriminator gradients\n","        disc_opt.zero_grad()\n","        # Get noise corresponding to the current batch_size \n","        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n","        \n","        # Get the images from the generator\n","        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n","        #        2) Generate the conditioned fake images\n","       \n","        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n","        fake = gen(noise_and_labels)\n","        \n","        # Make sure that enough images were generated\n","        assert len(fake) == len(real)\n","        # Check that correct tensors were combined\n","        assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1])\n","        # It comes from the correct generator\n","        assert tuple(fake.shape) == (len(real), 1, 28, 28)\n","\n","        # Get the predictions from the discriminator\n","        # Steps: 1) Create the input for the discriminator\n","        #           a) Combine the fake images with image_one_hot_labels, \n","        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n","        #           b) Combine the real images with image_one_hot_labels\n","        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n","        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n","        \n","        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n","        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n","        disc_fake_pred = disc(fake_image_and_labels).detach()\n","        disc_real_pred = disc(real_image_and_labels)\n","        \n","        # Make sure shapes are correct \n","        assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n","        assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n","        # Make sure that enough predictions were made\n","        assert len(disc_real_pred) == len(real)\n","        # Make sure that the inputs are different\n","        assert torch.any(fake_image_and_labels != real_image_and_labels)\n","        # Shapes must match\n","        assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)\n","        assert tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)\n","        \n","        \n","        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n","        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n","        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n","        disc_loss.backward(retain_graph=True)\n","        disc_opt.step() \n","\n","        # Keep track of the average discriminator loss\n","        discriminator_losses += [disc_loss.item()]\n","\n","        ### Update generator ###\n","        # Zero out the generator gradients\n","        gen_opt.zero_grad()\n","\n","        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n","        # This will error if you didn't concatenate your labels to your image correctly\n","        disc_fake_pred = disc(fake_image_and_labels)\n","        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n","        gen_loss.backward()\n","        gen_opt.step()\n","\n","        # Keep track of the generator losses\n","        generator_losses += [gen_loss.item()]\n","        #\n","\n","        if cur_step % display_step == 0 and cur_step \u003e 0:\n","            gen_mean = sum(generator_losses[-display_step:]) / display_step\n","            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n","            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n","            show_tensor_images(fake)\n","            show_tensor_images(real)\n","            step_bins = 20\n","            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n","            num_examples = (len(generator_losses) // step_bins) * step_bins\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Generator Loss\"\n","            )\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Discriminator Loss\"\n","            )\n","            plt.legend()\n","            plt.show()\n","        elif cur_step == 0:\n","            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n","        cur_step += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T17sDZ2oaHjZ"},"outputs":[],"source":["# Before you explore, you should put the generator\n","# in eval mode, both in general and so that batch norm\n","# doesn't cause you issues and is using its eval statistics\n","gen = gen.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVe4T3pLaKjC"},"outputs":[],"source":["import math\n","\n","### Change me! ###\n","n_interpolation = 9 # Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)\n","interpolation_noise = get_noise(1, z_dim, device=device).repeat(n_interpolation, 1)\n","\n","def interpolate_class(first_number, second_number):\n","    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)\n","    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)\n","\n","    # Calculate the interpolation vector between the two labels\n","    percent_second_label = torch.linspace(0, 1, n_interpolation)[:, None]\n","    interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label\n","\n","    # Combine the noise and the labels\n","    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))\n","    fake = gen(noise_and_labels)\n","    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n","\n","### Change me! ###\n","start_plot_number = 1 # Choose the start digit\n","### Change me! ###\n","end_plot_number = 5 # Choose the end digit\n","\n","plt.figure(figsize=(8, 8))\n","interpolate_class(start_plot_number, end_plot_number)\n","_ = plt.axis('off')\n","\n","### Uncomment the following lines of code if you would like to visualize a set of pairwise class \n","### interpolations for a collection of different numbers, all in a single grid of interpolations.\n","### You'll also see another visualization like this in the next code block!\n","# plot_numbers = [2, 3, 4, 5, 7]\n","# n_numbers = len(plot_numbers)\n","# plt.figure(figsize=(8, 8))\n","# for i, first_plot_number in enumerate(plot_numbers):\n","#     for j, second_plot_number in enumerate(plot_numbers):\n","#         plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)\n","#         interpolate_class(first_plot_number, second_plot_number)\n","#         plt.axis('off')\n","# plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n","# plt.show()\n","# plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpTZqTM3aNhY"},"outputs":[],"source":["n_interpolation = 9 # How many intermediate images you want + 2 (for the start and end image)\n","\n","# This time you're interpolating between the noise instead of the labels\n","interpolation_label = get_one_hot_labels(torch.Tensor([5]).long(), n_classes).repeat(n_interpolation, 1).float()\n","\n","def interpolate_noise(first_noise, second_noise):\n","    # This time you're interpolating between the noise instead of the labels\n","    percent_first_noise = torch.linspace(0, 1, n_interpolation)[:, None].to(device)\n","    interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise)\n","\n","    # Combine the noise and the labels again\n","    noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))\n","    fake = gen(noise_and_labels)\n","    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n","\n","# Generate noise vectors to interpolate between\n","### Change me! ###\n","n_noise = 5 # Choose the number of noise examples in the grid\n","plot_noises = [get_noise(1, z_dim, device=device) for i in range(n_noise)]\n","plt.figure(figsize=(8, 8))\n","for i, first_plot_noise in enumerate(plot_noises):\n","    for j, second_plot_noise in enumerate(plot_noises):\n","        plt.subplot(n_noise, n_noise, i * n_noise + j + 1)\n","        interpolate_noise(first_plot_noise, second_plot_noise)\n","        plt.axis('off')\n","plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n","plt.show()\n","plt.close()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMtzj5f3/cUvltPNYM/RIgb","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"15ae1e9b7fdb46df976b1ebec12226c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dfb7c3dd05f42e29ffdfd27ddc9b91b","placeholder":"​","style":"IPY_MODEL_700a6f8e058c4684b56e261492b2d782","value":" 36/469 [00:30\u0026lt;07:07,  1.01it/s]"}},"3916340b8be64bdebf6e3c56c5915772":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_763cda4d7da84dcbbf51ea3c031a91a5","IPY_MODEL_903dc4737adc425eb415a8a7e1c97fab","IPY_MODEL_15ae1e9b7fdb46df976b1ebec12226c9"],"layout":"IPY_MODEL_afc8acd06799495b8c92d7d6abd9ce8a"}},"6dfb7c3dd05f42e29ffdfd27ddc9b91b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"700a6f8e058c4684b56e261492b2d782":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"763cda4d7da84dcbbf51ea3c031a91a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92f34434e6a844229d935ad1e329d28b","placeholder":"​","style":"IPY_MODEL_94baf60a9ffb4e2da5621d9826b45f12","value":"  8%"}},"903dc4737adc425eb415a8a7e1c97fab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c37cb132e5bf47acabe08e80f89fe68f","max":469,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8a4a81a166145f4bb07685d1c4e4669","value":36}},"92f34434e6a844229d935ad1e329d28b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94baf60a9ffb4e2da5621d9826b45f12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afc8acd06799495b8c92d7d6abd9ce8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c37cb132e5bf47acabe08e80f89fe68f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a4a81a166145f4bb07685d1c4e4669":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}