{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1czVdIlqnImH"
   },
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KD3ZgLs80vY"
   },
   "source": [
    "### Goals\n",
    "In this notebook, you will write a generative model based on the paper [*Unpaired Image-to-Image Translation\n",
    "using Cycle-Consistent Adversarial Networks*](https://arxiv.org/abs/1703.10593) by Zhu et al. 2017, commonly referred to as CycleGAN.\n",
    "\n",
    "You will be training a model that can convert horses into zebras, and vice versa. Once again, the emphasis of the assignment will be on the loss functions. In order for you to see good outputs more quickly, you'll be training your model starting from a pre-trained checkpoint. You are also welcome to train it from scratch on your own, if you so choose.\n",
    "\n",
    "\n",
    "<!-- You will take the segmentations that you generated in the previous assignment and produce photorealistic images. -->\n",
    "\n",
    "### Learning Objectives\n",
    "1.   Implement the loss functions of a CycleGAN model.\n",
    "2.   Observe the two GANs used in CycleGAN.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU8DDM6l9rZb"
   },
   "source": [
    "## Getting Started\n",
    "You will start by importing libraries, defining a visualization function, and getting the pre-trained CycleGAN checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfkorNJrnmNO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Inspired by https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/datasets.py\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
    "        if len(self.files_A) > len(self.files_B):\n",
    "            self.files_A, self.files_B = self.files_B, self.files_A\n",
    "        self.new_perm()\n",
    "        assert len(self.files_A) > 0, \"Make sure you downloaded the horse2zebra images!\"\n",
    "\n",
    "    def new_perm(self):\n",
    "        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n",
    "        if item_A.shape[0] != 3: \n",
    "            item_A = item_A.repeat(3, 1, 1)\n",
    "        if item_B.shape[0] != 3: \n",
    "            item_B = item_B.repeat(3, 1, 1)\n",
    "        if index == len(self) - 1:\n",
    "            self.new_perm()\n",
    "        # Old versions of PyTorch didn't support normalization for different-channeled images\n",
    "        return (item_A - 0.5) * 2, (item_B - 0.5) * 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjFyvNTG1CqY"
   },
   "source": [
    "## Generator\n",
    "The code for a CycleGAN generator is much like Pix2Pix's U-Net with the addition of the residual block between the encoding (contracting) and decoding (expanding) blocks.\n",
    "\n",
    "![Diagram of a CycleGAN generator: composed of encoding blocks, residual blocks, then decoding blocks](CycleGAN_Generator.png)\n",
    "*Diagram of a CycleGAN generator: composed of encoding blocks, residual blocks, and then decoding blocks.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaWIMyhlDZg9"
   },
   "source": [
    "#### Residual Block\n",
    "Perhaps the most notable architectural difference between the U-Net you used for Pix2Pix and the architecture you're using for CycleGAN are the residual blocks. In CycleGAN, after the expanding blocks, there are convolutional layers where the output is ultimately added to the original input so that the network can change as little as possible on the image. You can think of this transformation as a kind of skip connection, where instead of being concatenated as new channels before the convolution which combines them, it's added directly to the output of the convolution. In the visualization below, you can imagine the stripes being generated by the convolutions and then added to the original image of the horse to transform it into a zebra. These skip connections also allow the network to be deeper, because they help with vanishing gradients issues that come when a neural network gets too deep and the gradients multiply in backpropagation to become very small; instead, these skip connections enable more gradient flow. A deeper network is often able to learn more complex features.\n",
    "\n",
    "\n",
    "![Residual block explanation: shows horse going through convolutions leading to stripes, added to the original horse image to get a zebra](residual_block.png)\n",
    "\n",
    "*Example of a residual block.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7XTRKqPSYpl"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualBlock Class:\n",
    "    Performs two convolutions and an instance normalization, the input is added\n",
    "    to this output to form the residual block output.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "        self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "        self.instancenorm = nn.InstanceNorm2d(input_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ResidualBlock: \n",
    "        Given an image tensor, completes a residual block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        original_x = x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.instancenorm(x)\n",
    "        return original_x + x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJhq8a9USnfc"
   },
   "source": [
    "#### Contracting and Expanding Blocks\n",
    "\n",
    "The rest of the generator code will otherwise be much like the code you wrote for the last assignment: Pix2Pix's U-Net. The primary changes are the use of instance norm instead of batch norm (which you may recall from StyleGAN), no dropout, and a stride-2 convolution instead of max pooling. Feel free to investigate the code if you're interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvY4ZNyUviY9"
   },
   "outputs": [],
   "source": [
    "class ContractingBlock(nn.Module):\n",
    "    '''\n",
    "    ContractingBlock Class\n",
    "    Performs a convolution followed by a max pool operation and an optional instance norm.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels, use_bn=True, kernel_size=3, activation='relu'):\n",
    "        super(ContractingBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=kernel_size, padding=1, stride=2, padding_mode='reflect')\n",
    "        self.activation = nn.ReLU() if activation == 'relu' else nn.LeakyReLU(0.2)\n",
    "        if use_bn:\n",
    "            self.instancenorm = nn.InstanceNorm2d(input_channels * 2)\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ContractingBlock: \n",
    "        Given an image tensor, completes a contracting block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class ExpandingBlock(nn.Module):\n",
    "    '''\n",
    "    ExpandingBlock Class:\n",
    "    Performs a convolutional transpose operation in order to upsample, \n",
    "        with an optional instance norm\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels, use_bn=True):\n",
    "        super(ExpandingBlock, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(input_channels, input_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        if use_bn:\n",
    "            self.instancenorm = nn.InstanceNorm2d(input_channels // 2)\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ExpandingBlock: \n",
    "        Given an image tensor, completes an expanding block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n",
    "                    for the skip connection\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class FeatureMapBlock(nn.Module):\n",
    "    '''\n",
    "    FeatureMapBlock Class\n",
    "    The final layer of a Generator - \n",
    "    maps each the output to the desired number of output channels\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "        output_channels: the number of channels to expect for a given output\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(FeatureMapBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=7, padding=3, padding_mode='reflect')\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of FeatureMapBlock: \n",
    "        Given an image tensor, returns it mapped to the desired number of channels.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vhuBbmqDnE3"
   },
   "source": [
    "#### CycleGAN Generator\n",
    "Finally, you can put all the blocks together to create your CycleGAN generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJzGSeMFD2CA"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    A series of 2 contracting blocks, 9 residual blocks, and 2 expanding blocks to \n",
    "    transform an input image into an image from the other class, with an upfeature\n",
    "    layer at the start and a downfeature layer at the end.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "        output_channels: the number of channels to expect for a given output\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2)\n",
    "        res_mult = 4\n",
    "        self.res0 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res1 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res2 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res3 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res4 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res5 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res6 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res7 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res8 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.expand2 = ExpandingBlock(hidden_channels * 4)\n",
    "        self.expand3 = ExpandingBlock(hidden_channels * 2)\n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of Generator: \n",
    "        Given an image tensor, passes it through the U-Net with residual blocks\n",
    "        and returns the output.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.res0(x2)\n",
    "        x4 = self.res1(x3)\n",
    "        x5 = self.res2(x4)\n",
    "        x6 = self.res3(x5)\n",
    "        x7 = self.res4(x6)\n",
    "        x8 = self.res5(x7)\n",
    "        x9 = self.res6(x8)\n",
    "        x10 = self.res7(x9)\n",
    "        x11 = self.res8(x10)\n",
    "        x12 = self.expand2(x11)\n",
    "        x13 = self.expand3(x12)\n",
    "        xn = self.downfeature(x13)\n",
    "        return self.tanh(xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "Generator (Generator)                    [1, 3, 256, 256]     [1, 3, 256, 256]     --                   True\n",
       "├─FeatureMapBlock (upfeature)            [1, 3, 256, 256]     [1, 64, 256, 256]    --                   True\n",
       "│    └─Conv2d (conv)                     [1, 3, 256, 256]     [1, 64, 256, 256]    9,472                True\n",
       "├─ContractingBlock (contract1)           [1, 64, 256, 256]    [1, 128, 128, 128]   --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 64, 256, 256]    [1, 128, 128, 128]   73,856               True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 128, 128, 128]   [1, 128, 128, 128]   --                   --\n",
       "│    └─ReLU (activation)                 [1, 128, 128, 128]   [1, 128, 128, 128]   --                   --\n",
       "├─ContractingBlock (contract2)           [1, 128, 128, 128]   [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 128, 128, 128]   [1, 256, 64, 64]     295,168              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res0)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res1)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res2)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res3)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res4)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res5)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res6)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res7)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ResidualBlock (res8)                   [1, 256, 64, 64]     [1, 256, 64, 64]     --                   True\n",
       "│    └─Conv2d (conv1)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─ReLU (activation)                 [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "│    └─Conv2d (conv2)                    [1, 256, 64, 64]     [1, 256, 64, 64]     590,080              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 256, 64, 64]     [1, 256, 64, 64]     --                   --\n",
       "├─ExpandingBlock (expand2)               [1, 256, 64, 64]     [1, 128, 128, 128]   --                   True\n",
       "│    └─ConvTranspose2d (conv1)           [1, 256, 64, 64]     [1, 128, 128, 128]   295,040              True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 128, 128, 128]   [1, 128, 128, 128]   --                   --\n",
       "│    └─ReLU (activation)                 [1, 128, 128, 128]   [1, 128, 128, 128]   --                   --\n",
       "├─ExpandingBlock (expand3)               [1, 128, 128, 128]   [1, 64, 256, 256]    --                   True\n",
       "│    └─ConvTranspose2d (conv1)           [1, 128, 128, 128]   [1, 64, 256, 256]    73,792               True\n",
       "│    └─InstanceNorm2d (instancenorm)     [1, 64, 256, 256]    [1, 64, 256, 256]    --                   --\n",
       "│    └─ReLU (activation)                 [1, 64, 256, 256]    [1, 64, 256, 256]    --                   --\n",
       "├─FeatureMapBlock (downfeature)          [1, 64, 256, 256]    [1, 3, 256, 256]     --                   True\n",
       "│    └─Conv2d (conv)                     [1, 64, 256, 256]    [1, 3, 256, 256]     9,411                True\n",
       "├─Tanh (tanh)                            [1, 3, 256, 256]     [1, 3, 256, 256]     --                   --\n",
       "========================================================================================================================\n",
       "Total params: 11,378,179\n",
       "Trainable params: 11,378,179\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 56.83\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 261.62\n",
       "Params size (MB): 45.51\n",
       "Estimated Total Size (MB): 307.92\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=Generator(3,3), \n",
    "        input_size=(1, 3, 256, 256), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6ndvjc_1KXx"
   },
   "source": [
    "## PatchGAN Discriminator\n",
    "\n",
    "Next, you will define the discriminator—a PatchGAN. It will be very similar to what you saw in Pix2Pix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nVuJPjV1f92"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Structured like the contracting path of the U-Net, the discriminator will\n",
    "    output a matrix of values classifying corresponding portions of the image as real or fake. \n",
    "    Parameters:\n",
    "        input_channels: the number of image input channels\n",
    "        hidden_channels: the initial number of discriminator convolutional filters\n",
    "    '''\n",
    "    def __init__(self, input_channels, hidden_channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_bn=False, kernel_size=4, activation='lrelu')\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2, kernel_size=4, activation='lrelu')\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4, kernel_size=4, activation='lrelu')\n",
    "        self.final = nn.Conv2d(hidden_channels * 8, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.contract3(x2)\n",
    "        xn = self.final(x3)\n",
    "        return xn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRk_8azSq3tF"
   },
   "source": [
    "## Training Preparation\n",
    "<!-- You'll be using the same U-Net as in the previous assignment, but you'll write another discriminator and change the loss to make it a GAN. -->\n",
    "\n",
    "Now you can put everything together for training! You will start by defining your parameters:\n",
    "\n",
    "  *   adv_criterion: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN\n",
    "  *   recon_criterion: a loss function that rewards similar images to the ground truth, which \"reconstruct\" the image\n",
    "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
    "  *   dim_A: the number of channels of the images in pile A\n",
    "  *   dim_B: the number of channels of the images in pile B (note that in the visualization this is currently treated as equivalent to dim_A)\n",
    "  *   display_step: how often to display/visualize the images\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   lr: the learning rate\n",
    "  *   target_shape: the size of the input and output images (in pixels)\n",
    "  *   load_shape: the size for the dataset to load the images at before randomly cropping them to target_shape as a simple data augmentation\n",
    "  *   device: the device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXptQZcwrBrq"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "adv_criterion = nn.MSELoss() \n",
    "recon_criterion = nn.L1Loss() \n",
    "\n",
    "n_epochs = 20\n",
    "dim_A = 3\n",
    "dim_B = 3\n",
    "display_step = 200\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "load_shape = 286\n",
    "target_shape = 256\n",
    "device = 'cuda'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPOUC6-nVDCv"
   },
   "source": [
    "You will then load the images of the dataset while introducing some data augmentation (e.g. crops and random horizontal flips). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNAK2XqMJ419"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(load_shape),\n",
    "    transforms.RandomCrop(target_shape),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "import torchvision\n",
    "dataset = ImageDataset(\"horse2zebra\", transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7vKN1POUjud"
   },
   "source": [
    "Next, you can initialize your generators and discriminators, as well as their optimizers. For CycleGAN, you will have two generators and two discriminators since there are two GANs:\n",
    "\n",
    "*   Generator for horse to zebra (`gen_AB`)\n",
    "*   Generator for zebra to horse (`gen_BA`)\n",
    "*   Discriminator for horse (`disc_A`)\n",
    "*   Discriminator for zebra (`disc_B`)\n",
    "\n",
    "You will also load your pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBY3Y9UrUgVX"
   },
   "outputs": [],
   "source": [
    "gen_AB = Generator(dim_A, dim_B).to(device)\n",
    "gen_BA = Generator(dim_B, dim_A).to(device)\n",
    "gen_opt = torch.optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "disc_A = Discriminator(dim_A).to(device)\n",
    "disc_A_opt = torch.optim.Adam(disc_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "disc_B = Discriminator(dim_B).to(device)\n",
    "disc_B_opt = torch.optim.Adam(disc_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Feel free to change pretrained to False if you're training the model from scratch\n",
    "pretrained = True\n",
    "if pretrained:\n",
    "    pre_dict = torch.load('cycleGAN_100000.pth')\n",
    "    gen_AB.load_state_dict(pre_dict['gen_AB'])\n",
    "    gen_BA.load_state_dict(pre_dict['gen_BA'])\n",
    "    gen_opt.load_state_dict(pre_dict['gen_opt'])\n",
    "    disc_A.load_state_dict(pre_dict['disc_A'])\n",
    "    disc_A_opt.load_state_dict(pre_dict['disc_A_opt'])\n",
    "    disc_B.load_state_dict(pre_dict['disc_B'])\n",
    "    disc_B_opt.load_state_dict(pre_dict['disc_B_opt'])\n",
    "else:\n",
    "    gen_AB = gen_AB.apply(weights_init)\n",
    "    gen_BA = gen_BA.apply(weights_init)\n",
    "    disc_A = disc_A.apply(weights_init)\n",
    "    disc_B = disc_B.apply(weights_init)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcpFbNDYzJrh"
   },
   "source": [
    "## Discriminator Loss\n",
    "First, you're going to be implementing the discriminator loss. This is the same as in previous assignments, so it should be a breeze :) Don't forget to detach your generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExsrHkBqNfLN"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_disc_loss\n",
    "def get_disc_loss(real_X, fake_X, disc_X, adv_criterion):\n",
    "    '''\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    Parameters:\n",
    "        real_X: the real images from pile X\n",
    "        fake_X: the generated images of class X\n",
    "        disc_X: the discriminator for class X; takes images and returns real/fake class X\n",
    "            prediction matrices\n",
    "        adv_criterion: the adversarial loss function; takes the discriminator \n",
    "            predictions and the target labels and returns a adversarial \n",
    "            loss (which you aim to minimize)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    fake = disc_X(fake_X).detach()\n",
    "    fake_loss = adv_criterion(fake, torch.zeros_like(fake))\n",
    "    real = disc_X(real_X)\n",
    "    real_loss = adv_criterion(real, torch.ones_like(real))\n",
    "    disc_loss = (fake_loss + real_loss ) / 2\n",
    "    #### END CODE HERE ####\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9pKn0wJ5szz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "test_disc_X = lambda x: x * 97\n",
    "test_real_X = torch.tensor(83.)\n",
    "test_fake_X = torch.tensor(89.)\n",
    "test_adv_criterion = lambda x, y: x * 79 + y * 73\n",
    "assert torch.abs((get_disc_loss(test_real_X, test_fake_X, test_disc_X, test_adv_criterion)) - 659054.5000) < 1e-6\n",
    "test_disc_X = lambda x: x.mean(0, keepdim=True)\n",
    "test_adv_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "test_input = torch.ones(20, 10)\n",
    "# If this runs, it's a pass - checks that the shapes are treated correctly\n",
    "get_disc_loss(test_input, test_input, test_disc_X, test_adv_criterion)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00gvvb5JWLTC"
   },
   "source": [
    "## Generator Loss\n",
    "While there are some changes to the CycleGAN architecture from Pix2Pix, the most important distinguishing feature of CycleGAN is its generator loss. You will be implementing that here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCX5SsevIdNK"
   },
   "source": [
    "#### Adversarial Loss\n",
    "The first component of the generator's loss you're going to implement is its adversarial loss—this once again is pretty similar to the GAN loss that you've implemented in the past. The important thing to note is that the criterion now is based on least squares loss, rather than binary cross entropy loss or W-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRRnnkwGwV5J"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_gen_adversarial_loss\n",
    "def get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion):\n",
    "    '''\n",
    "    Return the adversarial loss of the generator given inputs\n",
    "    (and the generated images for testing purposes).\n",
    "    Parameters:\n",
    "        real_X: the real images from pile X\n",
    "        disc_Y: the discriminator for class Y; takes images and returns real/fake class Y\n",
    "            prediction matrices\n",
    "        gen_XY: the generator for class X to Y; takes images and returns the images \n",
    "            transformed to class Y\n",
    "        adv_criterion: the adversarial loss function; takes the discriminator \n",
    "                  predictions and the target labels and returns a adversarial \n",
    "                  loss (which you aim to minimize)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    fake_Y = gen_XY(real_X)\n",
    "    output = disc_Y(fake_Y)\n",
    "    adversarial_loss = adv_criterion(output, torch.ones_like(output))\n",
    "    #### END CODE HERE ####\n",
    "    return adversarial_loss, fake_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FlbCSG29M20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "test_disc_Y = lambda x: x * 97\n",
    "test_real_X = torch.tensor(83.)\n",
    "test_gen_XY = lambda x: x * 89\n",
    "test_adv_criterion = lambda x, y: x * 79 + y * 73\n",
    "test_res = get_gen_adversarial_loss(test_real_X, test_disc_Y, test_gen_XY, test_adv_criterion)\n",
    "assert torch.abs(test_res[0] - 56606652) < 1e-6\n",
    "assert torch.abs(test_res[1] - 7387) < 1e-6\n",
    "test_disc_Y = lambda x: x.mean(0, keepdim=True)\n",
    "test_adv_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "test_input = torch.ones(20, 10)\n",
    "# If this runs, it's a pass - checks that the shapes are treated correctly\n",
    "get_gen_adversarial_loss(test_input, test_disc_Y, test_gen_XY, test_adv_criterion)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqsPfWxyIzto"
   },
   "source": [
    "#### Identity Loss\n",
    "Here you get to see some of the superbly new material! You'll want to measure the change in an image when you pass the generator an example from the target domain instead of the input domain it's expecting. The output should be the same as the input since it is already of the target domain class. For example, if you put a horse through a zebra -> horse generator, you'd expect the output to be the same horse because nothing needed to be transformed. It's already a horse! You don't want your generator to be transforming it into any other thing, so you want to encourage this behavior. In encouraging this identity mapping, the authors of CycleGAN found that for some tasks, this helped properly preserve the colors of an image, even when the expected input (here, a zebra) was put in. This was particularly useful for the photos <-> paintings mapping and, while an optional aesthetic component, you might find it useful for your applications down the line.\n",
    "\n",
    "![Diagram showing a real horse image going through a zebra -> horse generator and the ideal output being the same input image](Identity_Loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZU4PNEFB4r7M"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_identity_loss\n",
    "def get_identity_loss(real_X, gen_YX, identity_criterion):\n",
    "    '''\n",
    "    Return the identity loss of the generator given inputs\n",
    "    (and the generated images for testing purposes).\n",
    "    Parameters:\n",
    "        real_X: the real images from pile X\n",
    "        gen_YX: the generator for class Y to X; takes images and returns the images \n",
    "            transformed to class X\n",
    "        identity_criterion: the identity loss function; takes the real images from X and\n",
    "                        those images put through a Y->X generator and returns the identity \n",
    "                        loss (which you aim to minimize)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    identity_X = gen_YX(real_X)\n",
    "    identity_loss = identity_criterion(real_X, identity_X)\n",
    "    #### END CODE HERE ####\n",
    "    return identity_loss, identity_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-8pKidC92CL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "test_real_X = torch.tensor(83.)\n",
    "test_gen_YX = lambda x: x * 89\n",
    "test_identity_criterion = lambda x, y: (x + y) * 73\n",
    "test_res = get_identity_loss(test_real_X, test_gen_YX, test_identity_criterion)\n",
    "assert torch.abs(test_res[0] - 545310) < 1e-6\n",
    "assert torch.abs(test_res[1] - 7387) < 1e-6\n",
    "print(\"Success!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4riyUqIJJXS"
   },
   "source": [
    "#### Cycle Consistency Loss\n",
    "Now, you can implement the final generator loss and the part that puts the \"cycle\" in CycleGAN: cycle consistency loss. This is used to ensure that when you put an image through one generator, that if it is then transformed back into the input class using the opposite generator, the image is the same as the original input image.\n",
    "\n",
    "![Diagram showing a real zebra image being transformed into a horse and then back into a zebra. The output zebra should be the same as the input zebra.](Cycle_Consistency_Loss.png)\n",
    "\n",
    "Since you've already generated a fake image for the adversarial part, you can pass that fake image back to produce a full cycle—this loss will encourage the cycle to preserve as much information as possible.\n",
    "\n",
    "*Fun fact: Cycle consistency is a broader concept that's used outside of CycleGAN a lot too! It's helped with data augmentation and has been used on text translation too, e.g. French -> English -> French should get the same phrase back.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZE-Eyj0LOpm"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_cycle_consistency_loss\n",
    "def get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion):\n",
    "    '''\n",
    "    Return the cycle consistency loss of the generator given inputs\n",
    "    (and the generated images for testing purposes).\n",
    "    Parameters:\n",
    "        real_X: the real images from pile X\n",
    "        fake_Y: the generated images of class Y\n",
    "        gen_YX: the generator for class Y to X; takes images and returns the images \n",
    "            transformed to class X\n",
    "        cycle_criterion: the cycle consistency loss function; takes the real images from X and\n",
    "                        those images put through a X->Y generator and then Y->X generator\n",
    "                        and returns the cycle consistency loss (which you aim to minimize)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    cycle_X = gen_YX(fake_Y)\n",
    "    cycle_loss = cycle_criterion(cycle_X, real_X)\n",
    "    #### END CODE HERE ####\n",
    "    return cycle_loss, cycle_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07yvxXy0-NhH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "test_real_X = torch.tensor(83.)\n",
    "test_fake_Y = torch.tensor(97.)\n",
    "test_gen_YX = lambda x: x * 89\n",
    "test_cycle_criterion = lambda x, y: (x + y) * 73\n",
    "test_res = get_cycle_consistency_loss(test_real_X, test_fake_Y, test_gen_YX, test_cycle_criterion)\n",
    "assert torch.abs(test_res[1] - 8633) < 1e-6\n",
    "assert torch.abs(test_res[0] - 636268) < 1e-6\n",
    "print(\"Success!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RchMRsiyJuRi"
   },
   "source": [
    "#### Generator Loss (Total)\n",
    "\n",
    "Finally, you can put it all together! There are many components, so be careful as you go through this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oAdw-H1KbGv"
   },
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_gen_loss\n",
    "def get_gen_loss(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, identity_criterion, cycle_criterion, lambda_identity=0.1, lambda_cycle=10):\n",
    "    '''\n",
    "    Return the loss of the generator given inputs.\n",
    "    Parameters:\n",
    "        real_A: the real images from pile A\n",
    "        real_B: the real images from pile B\n",
    "        gen_AB: the generator for class A to B; takes images and returns the images \n",
    "            transformed to class B\n",
    "        gen_BA: the generator for class B to A; takes images and returns the images \n",
    "            transformed to class A\n",
    "        disc_A: the discriminator for class A; takes images and returns real/fake class A\n",
    "            prediction matrices\n",
    "        disc_B: the discriminator for class B; takes images and returns real/fake class B\n",
    "            prediction matrices\n",
    "        adv_criterion: the adversarial loss function; takes the discriminator \n",
    "            predictions and the true labels and returns a adversarial \n",
    "            loss (which you aim to minimize)\n",
    "        identity_criterion: the reconstruction loss function used for identity loss\n",
    "            and cycle consistency loss; takes two sets of images and returns\n",
    "            their pixel differences (which you aim to minimize)\n",
    "        cycle_criterion: the cycle consistency loss function; takes the real images from X and\n",
    "            those images put through a X->Y generator and then Y->X generator\n",
    "            and returns the cycle consistency loss (which you aim to minimize).\n",
    "            Note that in practice, cycle_criterion == identity_criterion == L1 loss\n",
    "        lambda_identity: the weight of the identity loss\n",
    "        lambda_cycle: the weight of the cycle-consistency loss\n",
    "    '''\n",
    "    # Hint 1: Make sure you include both directions - you can think of the generators as collaborating\n",
    "    # Hint 2: Don't forget to use the lambdas for the identity loss and cycle loss!\n",
    "    #### START CODE HERE ####\n",
    "    # Adversarial Loss -- get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion)\n",
    "    adversarial_loss_AB, fake_B = get_gen_adversarial_loss(real_A, disc_B, gen_AB, adv_criterion)\n",
    "    adversarial_loss_BA, fake_A = get_gen_adversarial_loss(real_B, disc_A, gen_BA, adv_criterion)\n",
    "    # Identity Loss -- get_identity_loss(real_X, gen_YX, identity_criterion)\n",
    "    identity_loss_BA, identity_A = get_identity_loss(real_A, gen_BA, identity_criterion)\n",
    "    identity_loss_AB, identity_B = get_identity_loss(real_B, gen_AB, identity_criterion)\n",
    "\n",
    "    # Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)\n",
    "    cycle_loss_AB, cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)\n",
    "    cycle_loss_BA, cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)\n",
    "    \n",
    "    gen_loss = adversarial_loss_AB + adversarial_loss_BA +\\\n",
    "                       lambda_identity *  (identity_loss_BA + identity_loss_AB) +\\\n",
    "                        lambda_cycle * (cycle_loss_AB + cycle_loss_BA )\n",
    "    # Total loss\n",
    "    #### END CODE HERE ####\n",
    "    return gen_loss, fake_A, fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sV_WpbEo-njW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "test_real_A = torch.tensor(97)\n",
    "test_real_B = torch.tensor(89)\n",
    "test_gen_AB = lambda x: x * 83\n",
    "test_gen_BA = lambda x: x * 79\n",
    "test_disc_A = lambda x: x * 47\n",
    "test_disc_B = lambda x: x * 43\n",
    "test_adv_criterion = lambda x, y: x * 73 + y * 71\n",
    "test_recon_criterion = lambda x, y: (x + y) * 61\n",
    "test_lambda_identity = 59\n",
    "test_lambda_cycle = 53\n",
    "test_res = get_gen_loss(\n",
    "    test_real_A, \n",
    "    test_real_B, \n",
    "    test_gen_AB, \n",
    "    test_gen_BA, \n",
    "    test_disc_A,\n",
    "    test_disc_B,\n",
    "    test_adv_criterion, \n",
    "    test_recon_criterion, \n",
    "    test_recon_criterion, \n",
    "    test_lambda_identity, \n",
    "    test_lambda_cycle)\n",
    "assert test_res[0].item() == 4047804560\n",
    "assert test_res[1].item() == 7031\n",
    "assert test_res[2].item() == 8051\n",
    "print(\"Success!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMDZWZTz3ivA"
   },
   "source": [
    "## CycleGAN Training\n",
    "\n",
    "Lastly, you can train the model and see some of your zebras, horses, and some that might not quite look like either! Note that this training will take a long time, so feel free to use the pre-trained checkpoint as an example of what a pretty-good CycleGAN does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fy6UBV60HtnY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f178cd0267e146b1b066f598aa5f8911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m     62\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_AB\u001b[39m\u001b[38;5;124m'\u001b[39m: gen_AB\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     63\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_BA\u001b[39m\u001b[38;5;124m'\u001b[39m: gen_BA\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisc_B_opt\u001b[39m\u001b[38;5;124m'\u001b[39m: disc_B_opt\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     69\u001b[0m                     }, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcycleGAN_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcur_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m             cur_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(save_model)\u001b[0m\n\u001b[1;32m     24\u001b[0m disc_A_opt\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zero out the gradient before backpropagation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     fake_A \u001b[38;5;241m=\u001b[39m \u001b[43mgen_BA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_B\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m disc_A_loss \u001b[38;5;241m=\u001b[39m get_disc_loss(real_A, fake_A, disc_A, adv_criterion)\n\u001b[1;32m     28\u001b[0m disc_A_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Update gradients\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Function for completing a forward pass of Generator: \u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Given an image tensor, passes it through the U-Net with residual blocks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m        x: image tensor of shape (batch size, channels, height, width)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontract1(x0)\n\u001b[1;32m     41\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontract2(x1)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mFeatureMapBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Function for completing a forward pass of FeatureMapBlock: \u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    Given an image tensor, returns it mapped to the desired number of channels.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m        x: image tensor of shape (batch size, channels, height, width)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reversed_padding_repeated_twice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skimage import color\n",
    "import numpy as np\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "\n",
    "def train(save_model=False):\n",
    "    mean_generator_loss = 0\n",
    "    mean_discriminator_loss = 0\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    cur_step = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Dataloader returns the batches\n",
    "        # for image, _ in tqdm(dataloader):\n",
    "        for real_A, real_B in tqdm(dataloader):\n",
    "            # image_width = image.shape[3]\n",
    "            real_A = nn.functional.interpolate(real_A, size=target_shape)\n",
    "            real_B = nn.functional.interpolate(real_B, size=target_shape)\n",
    "            cur_batch_size = len(real_A)\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "\n",
    "            ### Update discriminator A ###\n",
    "            disc_A_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad():\n",
    "                fake_A = gen_BA(real_B)\n",
    "            disc_A_loss = get_disc_loss(real_A, fake_A, disc_A, adv_criterion)\n",
    "            disc_A_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_A_opt.step() # Update optimizer\n",
    "\n",
    "            ### Update discriminator B ###\n",
    "            disc_B_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad():\n",
    "                fake_B = gen_AB(real_A)\n",
    "            disc_B_loss = get_disc_loss(real_B, fake_B, disc_B, adv_criterion)\n",
    "            disc_B_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_B_opt.step() # Update optimizer\n",
    "\n",
    "            ### Update generator ###\n",
    "            gen_opt.zero_grad()\n",
    "            gen_loss, fake_A, fake_B = get_gen_loss(\n",
    "                real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, recon_criterion, recon_criterion\n",
    "            )\n",
    "            gen_loss.backward() # Update gradients\n",
    "            gen_opt.step() # Update optimizer\n",
    "\n",
    "            # Keep track of the average discriminator loss\n",
    "            mean_discriminator_loss += disc_A_loss.item() / display_step\n",
    "            # Keep track of the average generator loss\n",
    "            mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "            ### Visualization code ###\n",
    "            if cur_step % display_step == 0:\n",
    "                print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
    "                show_tensor_images(torch.cat([real_A, real_B]), size=(dim_A, target_shape, target_shape))\n",
    "                show_tensor_images(torch.cat([fake_B, fake_A]), size=(dim_B, target_shape, target_shape))\n",
    "                mean_generator_loss = 0\n",
    "                mean_discriminator_loss = 0\n",
    "                # You can change save_model to True if you'd like to save the model\n",
    "                if save_model:\n",
    "                    torch.save({\n",
    "                        'gen_AB': gen_AB.state_dict(),\n",
    "                        'gen_BA': gen_BA.state_dict(),\n",
    "                        'gen_opt': gen_opt.state_dict(),\n",
    "                        'disc_A': disc_A.state_dict(),\n",
    "                        'disc_A_opt': disc_A_opt.state_dict(),\n",
    "                        'disc_B': disc_B.state_dict(),\n",
    "                        'disc_B_opt': disc_B_opt.state_dict()\n",
    "                    }, f\"cycleGAN_{cur_step}.pth\")\n",
    "            cur_step += 1\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1KD3ZgLs80vY"
   ],
   "name": "C3W3: CycleGAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "GANSC3-3A"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
