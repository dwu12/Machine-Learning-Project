{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LI6HORvzdcYu"
      },
      "source": [
        "# Neural Radiance Fields (NeRF)\n",
        "\n",
        "*Please note that this is an optional notebook meant to introduce more advanced concepts. If you’re up for a challenge, take a look and don’t worry if you can’t follow everything. There is no code to implement—only some cool code for you to learn and run!*\n",
        "\n",
        "### Goals\n",
        "\n",
        "In this notebook, you'll learn how to use Neural Radiance Fields to generate new views of a complex 3D scene using only a couple input views, first proposed by [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934) (Mildenhall et al. 2020). Though 2D GANs have seen success in high-resolution image synthesis, NeRF has quickly become a popular technique to enable high-resolution 3D-aware GANs.\n",
        "\n",
        "![Water fountain](https://raw.githubusercontent.com/alexxke/nerf-images/main/colorspout_200k_rgb.gif)\n",
        "![Water fountain](https://raw.githubusercontent.com/alexxke/nerf-images/main/orchid.gif)\n",
        "![Synthetic Lego Dataset](https://raw.githubusercontent.com/alexxke/nerf-images/main/lego.gif)\n",
        "\n",
        "Image Credit: [Matthew Tancik](https://www.matthewtancik.com/nerf) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n0_n9P6hObC0"
      },
      "source": [
        "## Overview\n",
        "\n",
        "NeRF is an approach for **novel view synthesis**, where given some input images of a scene and cooresponding camera poses, we want to generate new images of the same scene from arbitrary camera poses. Because training a full NeRF can take hours to days, we will study a feature-limited tiny NeRF ([official GitHub](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)) to train faster, while highlighting the major differences."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qj-W7lr2ivAG"
      },
      "source": [
        "### How does NeRF work?\n",
        "\n",
        "NeRF represents a scene as a function mapping position and direction to color and volumetric density (how opaque is this object?), $F_\\Theta : (x, y, z, \\theta, \\phi) \\mapsto (R, G, B, \\sigma)$. The authors then use these colors and densities with classic volume rendering techniques to compose these values into an image.\n",
        "\n",
        "NeRF represents this mapping with a simple Multilayer Perceptron (MLP), which is differentiable and thus allows for explicit optimization by comparing the synthesized with the ground truth images.\n",
        "\n",
        "![NeRF Pipeline](https://github.com/bmild/nerf/blob/master/imgs/pipeline.jpg?raw=true)\n",
        "Image credit: [Official GitHub](https://github.com/bmild/nerf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LhWMzgxV4Lmz"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJcfNEJ4BdN-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def batch_generator(inputs, batch_size):\n",
        "    \"\"\"\n",
        "    Generates batches of `batch_size` from `inputs` array.\n",
        "    \"\"\"\n",
        "    l = inputs.shape[0]\n",
        "    for i in range(0, l, batch_size):\n",
        "        yield inputs[i:min(i + batch_size, l)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xMurQzjRi7lC"
      },
      "source": [
        "Let's load our data from the official NeRF GitHub and plot a holdout image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "XMje4aK7ZZXb",
        "outputId": "8e3be4d1-aa42-4132-d274-e8cda7d3a31a"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "    !wget https://bmild.github.io/nerf/tiny_nerf_data.npz\n",
        "\n",
        "data = np.load('tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "print(images.shape, poses.shape, focal)\n",
        "\n",
        "testimg, testpose = images[101], poses[101]\n",
        "# use the first 100 images for training\n",
        "images = images[:100,...,:3]\n",
        "poses = poses[:100]\n",
        "\n",
        "plt.imshow(testimg)\n",
        "plt.show()\n",
        "\n",
        "images = torch.from_numpy(images).to(device)\n",
        "poses = torch.from_numpy(poses).to(device)\n",
        "testimg = torch.from_numpy(testimg).to(device)\n",
        "testpose = torch.from_numpy(testpose).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IAQVcGbSRHG-"
      },
      "source": [
        "We define a function to compute the rays passing through an image from a given position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwTUrkq1fUJU"
      },
      "outputs": [],
      "source": [
        "def get_rays(height, width, focal_length, cam2world):\n",
        "    \"\"\"\n",
        "    Compute the rays (origins and directions) passing through an image with \n",
        "    `height` and `width` (in pixels). `focal_length` (in pixels) is a property \n",
        "    of the camera. `cam2world` represents and transform tensor from a 3D point\n",
        "    in the \"camera\" frame of reference to the \"world\" frame of reference (the \n",
        "    `pose` in our dataset).\n",
        "    \"\"\"\n",
        "    i, j = torch.meshgrid(\n",
        "        torch.arange(width).to(cam2world),\n",
        "        torch.arange(height).to(cam2world),\n",
        "        indexing=\"xy\"\n",
        "    )\n",
        "    dirs = torch.stack([\n",
        "        (i.cpu() - width / 2) / focal_length,\n",
        "        - (j.cpu() - height / 2) / focal_length,\n",
        "        - torch.ones_like(i.cpu())\n",
        "    ], dim=-1).to(cam2world)\n",
        "    rays_d = torch.sum(dirs[..., None, :] * cam2world[:3, :3], dim=-1)\n",
        "    rays_o = cam2world[:3, -1].expand(rays_d.shape)\n",
        "    return rays_o, rays_d"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f_iZYbQtOW2s"
      },
      "source": [
        "## Building NeRF\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iHpreQ_wDKe1"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "To better model high-frequency functions, the authors use an encoding function defined\n",
        "\n",
        "$$\\gamma(p) = (\\sin(2^0 \\pi p), \\cos(2^0 \\pi p), \\dots, \\sin(2^{L-1} \\pi p), \\cos(2^{L-1} \\pi p)).$$\n",
        "\n",
        "In the NeRF architecture, $\\gamma$ is applied to each of the 5 input dimensions $(x, y, z, \\theta, \\phi)$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3MuDPrzEgfQ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(x, L_embed=6):\n",
        "    \"\"\"\n",
        "    Returns tensor representing positional encoding $\\gamma(x)$ of `x` with\n",
        "    `L_embed` corresponding to $L$ in the above.\n",
        "    \"\"\"\n",
        "    rets = [x]\n",
        "    for i in range(L_embed):\n",
        "        for fn in [torch.sin, torch.cos]:\n",
        "            rets.append(fn(2 ** i * x))\n",
        "    return torch.cat(rets, dim=-1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qeIvKF6ZdFN3"
      },
      "source": [
        "### Architecture\n",
        "\n",
        "NeRF is a simple MLP with ReLU activations, concatenations at specific layers, and outputs at different stages shown below:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1129/1*q3fLvJFfoUdtVhsXeeTNXw.png\" alt=\"NeRF Architecture\" width=\"800\"/>\n",
        "\n",
        "For training speed, we instead implement a smaller MLP in the same spirit:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0gx9TeKB7vF"
      },
      "outputs": [],
      "source": [
        "class TinyNeRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements 4 layer MLP as a tiny example of the NeRF design\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=128, L_embed=6):\n",
        "        super().__init__()\n",
        "        in_dim = 3 + 3 * 2 * L_embed\n",
        "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.layer3 = nn.Linear(hidden_dim + in_dim, hidden_dim)\n",
        "        self.layer4 = nn.Linear(hidden_dim, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.layer1(x))\n",
        "        out = F.relu(self.layer2(out))\n",
        "        out = F.relu(self.layer3(torch.cat([out, x], dim=-1)))\n",
        "        out = self.layer4(out)\n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PdvFCdEORmLJ"
      },
      "source": [
        "### Volume Rendering\n",
        "\n",
        "Consider a camera ray $\\mathbf r(t) = \\mathbf o + t \\mathbf d$, with origin $\\mathbf o$ and direction $\\mathbf d$. If each 3D point along this ray is associated with a color $\\mathbf c(\\mathbf r(t), \\mathbf d)$ and density $\\sigma(\\mathbf r(t)$, then the authors render the expected color at $\\mathbf r(t)$ as\n",
        "\n",
        "$$C(\\mathbf r) = \\int_{t_n}^{t_f} T(t) \\sigma(\\mathbf r(t)) \\mathbf c(\\mathbf r(t), \\mathbf d) dt,$$\n",
        "\n",
        "where\n",
        "\n",
        "$$T(t) = \\exp \\left( - \\int_{t_n}^t \\sigma(\\mathbf r(s)) ds \\right),$$\n",
        "\n",
        "and $t_n$ and $t_f$ are the near and far bounds of what we wish to render. We can interpret $T(t)$ as the probability that the ray travels from $t_n$ to $t$ without hitting any other particle.\n",
        "\n",
        "This formulation is continuous, so the authors discretize it with stratified sampling: divide the interval $[t_n, t_f]$ into $N$ even bins, and then sample uniformly over each bin. The sampling is critical so we can roughly cover the whole interval over the course of training. This yields the discretization\n",
        "\n",
        "$$\\hat C(\\mathbf r) = \\sum_{i=1}^N T_i (1 - \\exp(- \\sigma_i \\delta_i)) \\mathbf c_i,$$\n",
        "\n",
        "with\n",
        "\n",
        "$$T_i = \\exp \\left( - \\sum_{j=1}^{i-1} \\sigma_j \\delta_j \\right),$$\n",
        "\n",
        "and $\\delta_i = t_{i+1} - t_i$ as the difference series between sample points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XtlYS7WRuK8"
      },
      "outputs": [],
      "source": [
        "def render_rays(\n",
        "    model, rays_o, rays_d, near, far, N_samples, encoding_fn, rand=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Use `model` to render the rays parameterized by `rays_o` and `rays_d`\n",
        "    between `near` and `far` limits with `N_samples`.\n",
        "    \"\"\"\n",
        "    # sample query pts\n",
        "    z_vals = torch.linspace(near, far, N_samples).to(rays_o)\n",
        "    if rand:\n",
        "        z_vals = (\n",
        "            torch.rand(list(rays_o.shape[:-1]) + [N_samples]) \n",
        "            * (far - near) / N_samples\n",
        "        ).to(rays_o) + z_vals\n",
        "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
        "\n",
        "    # run query pts through model to get radiance fields\n",
        "    pts_flat = pts.reshape((-1, 3))\n",
        "    encoded_pts_flat = encoding_fn(pts_flat)\n",
        "    batches = batch_generator(encoded_pts_flat, batch_size=BATCH_SIZE)\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        preds.append(model(batch))\n",
        "    radiance_fields_flat = torch.cat(preds, dim=0)\n",
        "    radiance_fields = torch.reshape(\n",
        "        radiance_fields_flat, list(pts.shape[:-1]) + [4]\n",
        "    )\n",
        "\n",
        "    # compute densities and colors\n",
        "    sigma_a = F.relu(radiance_fields[..., 3])\n",
        "    rgb = torch.sigmoid(radiance_fields[..., :3])\n",
        "\n",
        "    # do volume rendering\n",
        "    oneE10 = torch.tensor([1e10], dtype=rays_o.dtype, device=rays_o.device)\n",
        "    dists = torch.cat([\n",
        "        z_vals[..., 1:] - z_vals[..., :-1],\n",
        "        oneE10.expand(z_vals[..., :1].shape)\n",
        "    ], dim=-1)\n",
        "    alpha = 1 - torch.exp(-sigma_a * dists)\n",
        "    weights = torch.roll(torch.cumprod(1 - alpha + 1e-10, dim=-1), 1, dims=-1)\n",
        "    weights[..., 0] = 1\n",
        "    weights = alpha * weights\n",
        "\n",
        "    rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n",
        "    depth_map = (weights * z_vals).sum(dim=-1)\n",
        "    acc_map = weights.sum(dim=-1)\n",
        "    return rgb_map, depth_map, acc_map"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iO-9QkfmGs2e"
      },
      "source": [
        "### Loss Function and Optimizer\n",
        "\n",
        "The authors formulate a simple loss function as the total squared error between the rendered and ground truth RGB image values. The authors use the Adam optimizer with default parameters and learning rate that begins at $5 \\times 10^{-4}$ and exponentially decays to $5 \\times 10^{-5}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-hb622RUE0Wz"
      },
      "source": [
        "### Modifications\n",
        "\n",
        "* **Input dimension**: Unlike the 5D input of the original NeRF model, our TinyNeRF only uses 3 input dimensions $(x, y, z)$, omitting the camera angle.\n",
        "* **Architecture**: We simplified the NeRF architecture to train faster.\n",
        "* **Optimizer**: We use the default Adam optimizer (lr= $5 \\times 10^{-3}$) without exponential decay.\n",
        "* **Hierarchical Volume Sampling**: Because sampling uniformly across a camera ray is not efficient (since much of the ray does not intersect with the object) NeRF actually uses a coarse network to determine where to sample, and then a fine network that samples from points that will contribute most to the output image. We use the simpler stratified sampling.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8xSVMKjIR8Dh"
      },
      "source": [
        "## Training NeRF"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdk47Fis1ym2"
      },
      "source": [
        "Finally we train NeRF! The training cell below should take around five minutes to run (with GPU runtime)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hTj3DVGZRv_3",
        "outputId": "7603453d-1485-4ffd-da18-ace056cd23b6"
      },
      "outputs": [],
      "source": [
        "# define parameters\n",
        "NUM_ENCODING_FUNCTIONS = 6\n",
        "NEAR = 2\n",
        "FAR = 6\n",
        "DEPTH_SAMPLES = 64\n",
        "LEARNING_RATE = 5e-3\n",
        "BATCH_SIZE = 16384\n",
        "NUM_EPOCHS = 1000\n",
        "DISPLAY_EVERY = 100\n",
        "HEIGHT, WIDTH = images.shape[1:3]\n",
        "FOCAL = data['focal']\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# initialize encoding function, model, loss, and optimizer\n",
        "encoding_fn = lambda x: positional_encoding(x, L_embed=NUM_ENCODING_FUNCTIONS)\n",
        "model = TinyNeRF(L_embed=NUM_ENCODING_FUNCTIONS)\n",
        "model.to(device)\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# for plotting the loss and iteration during training\n",
        "psnrs = []\n",
        "iternums = []\n",
        "\n",
        "for i in range(NUM_EPOCHS + 1):\n",
        "    # sample an image from our training set\n",
        "    img_idx = np.random.randint(images.shape[0]) \n",
        "    target = images[img_idx].to(device)\n",
        "    pose = poses[img_idx].to(device)\n",
        "\n",
        "    # get the rays passing through the image and forward pass the model\n",
        "    rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, pose)\n",
        "    rgb, _, _ = render_rays(\n",
        "        model, rays_o, rays_d, near=NEAR, far=FAR, N_samples=DEPTH_SAMPLES,\n",
        "        encoding_fn=encoding_fn\n",
        "    )\n",
        "\n",
        "    # backward pass\n",
        "    loss = loss_fn(rgb, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # plot the model's render of the test image and loss at each iteration\n",
        "    if i % DISPLAY_EVERY == 0:\n",
        "        rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, testpose)\n",
        "        rgb, _, _ = render_rays(\n",
        "            model, rays_o, rays_d, near=NEAR, far=FAR, N_samples=DEPTH_SAMPLES,\n",
        "            encoding_fn=encoding_fn\n",
        "        )\n",
        "        loss = loss_fn(rgb, testimg)\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "        psnr = -10 * torch.log10(loss)\n",
        "        psnrs.append(psnr.item())\n",
        "        iternums.append(i)\n",
        "\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(rgb.detach().cpu().numpy())\n",
        "        plt.title(f'Iteration: {i}')\n",
        "        plt.subplot(122)\n",
        "        plt.plot(iternums, psnrs)\n",
        "        plt.title('PSNR')\n",
        "        plt.show()\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "copnlKyQ8DoC"
      },
      "source": [
        "### Render 3D Video\n",
        "\n",
        "Looks great! Let's make a video of our result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "413mojvo880z"
      },
      "outputs": [],
      "source": [
        "# define some transformation tensors for translations and rotations about\n",
        "# different axes\n",
        "trans_t = lambda t : torch.tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,t],\n",
        "    [0,0,0,1],\n",
        "], dtype=torch.float32)\n",
        "\n",
        "rot_phi = lambda phi : torch.tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,np.cos(phi),-np.sin(phi),0],\n",
        "    [0,np.sin(phi), np.cos(phi),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=torch.float32)\n",
        "\n",
        "rot_theta = lambda th : torch.tensor([\n",
        "    [np.cos(th),0,-np.sin(th),0],\n",
        "    [0,1,0,0],\n",
        "    [np.sin(th),0, np.cos(th),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=torch.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    \"\"\"\n",
        "    Compute a transformation tensor for a spherical coordinates\n",
        "    (`theta`, `phi`, `radius`)\n",
        "    \"\"\"\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
        "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
        "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w.numpy()\n",
        "    return c2w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-86_Y3_9R8Z",
        "outputId": "057d1885-6985-4f3a-fd94-e3f651a24d59"
      },
      "outputs": [],
      "source": [
        "# run poses that encircle the object through our trained model and make a video\n",
        "frames = []\n",
        "for th in np.linspace(0., 360., 120, endpoint=False):\n",
        "    c2w = pose_spherical(th, -30, 4)\n",
        "    c2w = torch.from_numpy(c2w).to(device).float()\n",
        "    rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, c2w[:3,:4])\n",
        "    rgb, _, _ = render_rays(\n",
        "        model, rays_o, rays_d, NEAR, FAR, N_samples=DEPTH_SAMPLES,\n",
        "        encoding_fn=encoding_fn\n",
        "    )\n",
        "    frames.append((255*np.clip(rgb.cpu().detach().numpy(),0,1)).astype(np.uint8))\n",
        "\n",
        "import imageio\n",
        "f = 'video.mp4'\n",
        "imageio.mimwrite(f, frames, fps=30, quality=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "JJLKD0dY_Vgw",
        "outputId": "82146a43-e9fc-4bc0-acfa-7ba085977278"
      },
      "outputs": [],
      "source": [
        "# embed the video in the notebook\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ibeCS8GBSBHr"
      },
      "source": [
        "## Extensions\n",
        "\n",
        "Tying this back to GANs, many papers have found success in applying NeRFs to 3D aware GANs. This is a very active research area, so here's a couple pointers:\n",
        "\n",
        "* [GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis](https://arxiv.org/abs/2007.02442) (Schwarz et al. 2020) demonstrated a NeRF model that can generate classes of objects by conditioning on shape and appearance.\n",
        "\n",
        "* [pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis](https://arxiv.org/abs/2012.00926) (Chan et al. 2021) proposed the current, as of writing, state-of-the-art 3D aware image synthesis technique through more expressive architectures. A comparison of these techniques is below, credit [Marco Monteiro](https://marcoamonteiro.github.io/pi-GAN-website/).\n",
        "\n",
        "    ![Comparison of 3D-aware GANs](https://raw.githubusercontent.com/alexxke/nerf-images/main/comparison.gif)\n",
        "\n",
        "* [GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields](https://arxiv.org/abs/2011.12100) (Niemeyer and Geiger 2021) developed an approach that can model multi-object scenes as compositions of NeRFs. Check out some visualizations of object translation from a 2D GAN (left) and GIRAFFE (right), credit to [Michael Niemeyer](https://m-niemeyer.github.io/project-pages/giraffe/index.html).\n",
        "\n",
        "    ![2D GAN single object translation](https://raw.githubusercontent.com/alexxke/nerf-images/main/2dgan.gif)\n",
        "![GIRAFFE single object translation](https://raw.githubusercontent.com/alexxke/nerf-images/main/giraffe.gif)\n",
        "\n",
        "* [Block-NeRF: Scalable Large Scene Neural View Synthesis](https://arxiv.org/abs/2202.05263) (Tancik et al. 2022) takes scene decomposition even further by introducing tweaks that allow Block-NeRF to render an entire neighborhood of San Francisco (credit to [Waymo Research](https://waymo.com/research/block-nerf/)).\n",
        "\n",
        "    ![Block-NeRF on Grace Cathedral](https://raw.githubusercontent.com/alexxke/nerf-images/main/grace_cathedral.gif)\n",
        "\n",
        "\n",
        "For more general improvements on this technique, Frank Dellaert put together a great [anthology](https://dellaert.github.io/NeRF/)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
