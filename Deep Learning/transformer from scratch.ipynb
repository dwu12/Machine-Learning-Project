{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6wYzbywUhR9qk2fwt1eZ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ohue4Yxj22m1"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np"]},{"cell_type":"code","source":["class ScaledDotProductAttention(nn.Module):\n","  def __init__(self, dropout):\n","    super(ScaledDotProductAttention, self).__init__()\n","    self.dropout = nn.Dropout(dropout)\n","    self.softmax = nn.Softmax(dim = -1)\n","\n","  def forward(self, query , key, value, mask = None):\n","    # query : [batchsize, query_len, dim]\n","    # key   : [batchsize, key_len ,  dim]\n","    # value : [batchsize, value_len, dim]\n","    dim = query.shape[2]\n","\n","    score = torch.bmm(query, key.transpose(1,2)) / np.sqrt(dim) # [batchsize, query_len, key_len]\n","\n","    if mask is not None:\n","      score = score.masked_fill(mask == 0, -1e10)\n","\n","    attention_weight = self.softmax(score)\n","    return torch.bmm(self.dropout(attention_weight), value) # [batchsize, query_len, dim]\n"],"metadata":{"id":"jYg6w17pQkT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, embed_size, heads, dropout=0.1):\n","    super(MultiHeadAttention, self).__init__()\n","    self.heads = heads\n","    self.heads_dim = embed_size // heads\n","    self.attention = ScaledDotProductAttention(dropout)\n","    self.query = nn.Linear(embed_size, embed_size)\n","    self.key = nn.Linear(embed_size , embed_size)\n","    self.value = nn.Linear(embed_size, embed_size)\n","    self.fc = nn.Linear(embed_size, embed_size)\n","\n","  def forward(self, query , key , value, mask):\n","     # query : [batchsize, query_len, dim]\n","     # key   : [batchsize, key_len ,  dim]\n","     # value : [batchsize, value_len, dim]\n","     querys = self.query(query) #[batchsize, query_len, embed_size]\n","     keys = self.key(key)       #[batchsize, key_len,   embed_size]\n","     values = self.value(value) #[batchsize, value_len, embed_size]\n","\n","     querys = querys.reshape(querys.shape[0], querys.shape[1], self.heads, self.heads_dim)\n","     # [batchsize, query_len, self.heads, self.heads_dim]\n","     querys = querys.permute(0,2,1,3) #[batchsize, self.heads, query_len, self.heads_dim]\n","     querys = querys.reshape(-1, querys.shape[2], querys.shape[3]) #[batchsize*self.heads, query_len, self.heads_dim]\n","\n","     keys = keys.reshape(keys.shape[0], keys.shape[1], self.heads, self.heads_dim)\n","     # [batchsize, key_len, self.heads, self.heads_dim]\n","     keys = keys.permute(0,2,1,3) #[batchsize, self.heads, keys_len, self.heads_dim]\n","     keys = keys.reshape(-1, keys.shape[2], keys.shape[3]) #[batchsize*self.heads, key_len, self.heads_dim]\n","\n","     values = values.reshape(values.shape[0], values.shape[1], self.heads, self.heads_dim)\n","     # [batchsize, value_len, self.heads, self.heads_dim]\n","     values = values.permute(0,2,1,3) #[batchsize, self.heads, value_len, self.heads_dim]\n","     values = values.reshape(-1, values.shape[2], values.shape[3]) #[batchsize*self.heads, value_len, self.heads_dim]\n","\n","     output = self.attention(querys, keys, values, mask) \n","     #[batchsize * self.heads, query_len, self.heads_dim]\n","\n","     output = output.reshape(-1, self.heads, output.shape[1], output.shape[2])\n","     #[batchsize, self.heads, query_len, self.heads_dim]\n","\n","     output = output.permute(0, 2, 1, 3)\n","     #[batchsize, query_len, self.heads,  self.heads_dim]\n","\n","     output = output.reshape(output.shape[0],output.shape[1], -1)\n","     #[batchsize, query_len, embed_size]\n","\n","     return output\n","\n"],"metadata":{"id":"gG7q6zwhaM6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","  def __init__(self, embed_size, heads, dropout=0.1 , dim_feedforward = 2048):\n","    super(TransformerBlock, self).__init__()\n","    self.attention = MultiHeadAttention(embed_size, heads, dropout)\n","    self.norm1 = nn.LayerNorm(embed_size)\n","    self.norm2 = nn.LayerNorm(embed_size)\n","    self.feed_forward = nn.Sequential(\n","        nn.Linear(embed_size, dim_feedforward), \n","        nn.ReLU(),\n","        nn.Linear(dim_feedforward , embed_size)\n","    )\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, query, key, value, mask):\n","    # query : [batchsize, query_len, dim]\n","    # key   : [batchsize, key_len ,  dim]\n","    # value : [batchsize, value_len, dim]\n","    attention = self.attention(query, key, value, mask) #[batchsize, query_len, embed_size]\n","    x = self.dropout( self.norm1(attention + query) )   #[batchsize, query_len, embed_size]\n","    forward = self.feed_forward(x)                      #[batchsize, query_len, embed_size]\n","    output = self.dropout(self.norm2(forward + x))      #[batchsize, query_len, embed_size]\n","\n","    return output"],"metadata":{"id":"cVCHwuco7Q9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","  def __init__(self, embed_size, heads, dim_feedforward , dropout):\n","    super(DecoderLayer, self).__init__()\n","    self.attention = MultiHeadAttention(embed_size, heads, dropout)\n","    self.norm = nn.LayerNorm(embed_size)\n","    self.TransformerBlock = TransformerBlock(embed_size, heads, dropout, dim_feedforward)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, query, value, key, trg_mask, src_mask):\n","    attention = self.attention(query,query,query, trg_mask)\n","    query = self.dropout(self.norm(attention + query))\n","    output = self.TransformerBlock(query, key, value, src_mask)\n","    return output #[batchsize, query_len, embed_size]"],"metadata":{"id":"WpjtjnVR-Eef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, \n","               vocab_size,\n","               embed_size,\n","               num_layers,\n","               heads,\n","               dim_feedforward,\n","               dropout,\n","               device):\n","    super(Decoder, self).__init__()\n","    self.embed_size = embed_size\n","\n","    self.word_embedding = nn.Embedding(vocab_size, embed_size)\n","    \n","    self.layers = nn.ModuleList(\n","        [DecoderLayer(embed_size, heads, dim_feedforward, dropout) for _ in range(num_layers)]\n","    )\n","\n","    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, enc_out, trg_mask, src_mask):\n","    # x shape [batch_size, seq_length]\n","    batch_size, seq_length = x.shape\n","    \n","    positional_embedding = nn.Parameter(torch.randn(1, seq_length, self.embed_size))\n","\n","    x = self.dropout((positional_embedding + self.word_embedding(x)))\n","\n","    for layer in self.layers:\n","      x = layer(x, enc_out, enc_out, trg_mask, src_mask)\n","\n","    out = self.fc_out(x)\n","\n","    return out"],"metadata":{"id":"bhblkojf_Psd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A = Decoder()"],"metadata":{"id":"O_0XFtSyc8Ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nn.Parameter(torch.randn(1, 15, 256))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQEQx9wzcMoD","executionInfo":{"status":"ok","timestamp":1667008384461,"user_tz":240,"elapsed":153,"user":{"displayName":"DI WU","userId":"00334041348669656891"}},"outputId":"fb3ef7c0-6891-4359-b307-7d56c1585ec2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 15, 256])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","  def __init__(\n","      self, \n","      src_vocab_size,\n","      trg_vocab_size,\n","      src_pad_idx,\n","      trg_pad_idx,\n","      embed_size = 256,\n","      num_layer = 6,\n","      forward_expansion = 4,\n","      heads = 8,\n","      dropout = 0,\n","      device = \"cuda\",\n","      max_length = 100\n","  ):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(\n","        src_vocab_size,\n","        embed_size,\n","        num_layer,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length\n","    )\n","\n","    self.decoder = Decoder(\n","        trg_vocab_size,\n","        embed_size,\n","        num_layer,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length\n","    )\n","\n","    self.src_pad_idx = src_pad_idx\n","    self.trg_pad_idx = trg_pad_idx\n","\n","    self.device = device\n","\n","  def make_src_mask(self, src):\n","    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","    # (N, 1, 1, src_len)\n","    return src_mask.to(self.device)\n","\n","  def make_trg_mask(self, trg):\n","    N, trg_len = trg.shape\n","\n","    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1 ,trg_len, trg_len)\n","    return trg_mask.to(self.device)\n","\n","  def forward(self, src, trg):\n","    src_mask = self.make_src_mask(src)\n","    trg_mask = self.make_trg_mask(trg)\n","    enc_src = self.encoder(src, src_mask)\n","    out = self.decoder(trg,enc_src, src_mask, trg_mask)\n","    return out\n","    "],"metadata":{"id":"wzltcFf-Aa-J"},"execution_count":null,"outputs":[]}]}