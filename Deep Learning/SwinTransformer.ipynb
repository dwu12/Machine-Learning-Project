{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as f\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "    \n",
    "class PreNorm(nn.Module):\n",
    "    ## in swin v2, we use post norm , so we can change the forward\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        #return self.fn(self.norm(x), **kwargs) ## pernorm for version 1\n",
    "        return self.norm(self.fn(x, **kwargs))  ## postnorm for version 2\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    ## hidden_dim is the 4 * input_channels\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18.],\n",
      "        [19., 20., 21., 22., 23., 24., 25., 26., 27.],\n",
      "        [28., 29., 30., 31., 32., 33., 34., 35., 36.],\n",
      "        [37., 38., 39., 40., 41., 42., 43., 44., 45.],\n",
      "        [46., 47., 48., 49., 50., 51., 52., 53., 54.],\n",
      "        [55., 56., 57., 58., 59., 60., 61., 62., 63.],\n",
      "        [64., 65., 66., 67., 68., 69., 70., 71., 72.],\n",
      "        [73., 74., 75., 76., 77., 78., 79., 80., 81.]])\n",
      "-----------\n",
      "tensor([[11., 12., 13., 14., 15., 16., 17., 18., 10.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 19.],\n",
      "        [29., 30., 31., 32., 33., 34., 35., 36., 28.],\n",
      "        [38., 39., 40., 41., 42., 43., 44., 45., 37.],\n",
      "        [47., 48., 49., 50., 51., 52., 53., 54., 46.],\n",
      "        [56., 57., 58., 59., 60., 61., 62., 63., 55.],\n",
      "        [65., 66., 67., 68., 69., 70., 71., 72., 64.],\n",
      "        [74., 75., 76., 77., 78., 79., 80., 81., 73.],\n",
      "        [ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "#### test CyclicShift  ########\n",
    "\n",
    "x = torch.linspace(1, 81, 81).view(9,9)\n",
    "print(x)\n",
    "\n",
    "print('-----------')\n",
    "y = torch.roll(input = x, shifts = (-1, -1), dims = (0,1))\n",
    "print(y)\n",
    "\n",
    "#### end test CyclicShift #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### test create_mask  ############\n",
    "create_mask(window_size = 3 , displacement = 1, upper_lower= False, left_right = True)\n",
    "#### end test create_mask  ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "\n",
    "        ## dim = hidden_dim = (96, 192, 384, 768)\n",
    "        ## head = (3, 6, 12, 24)\n",
    "        ## head_dim  = 32\n",
    "\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads  ## hidden_dim [96, 192, 384, 768]\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5 ## like in transformer, we add 1/sqrt(d)\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted ## SW-MSA else W-MSA\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2 ## define how many pixel to shift: usually half of the window\n",
    "            self.cyclic_shift = CyclicShift(-displacement)     ## cycleshift\n",
    "            self.cyclic_back_shift = CyclicShift(displacement) ## shift back\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            ## absolute positional embedding\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## x shape:  [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)   ##  [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        ## after self.to_qkv(x) shape is:  [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (288, 576, 1152, 2304)]\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) ## divided three distict tensor based on the last dim\n",
    "        ### qkv[0] : [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        ### qkv[1] : [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        ### qkv[2] : [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "\n",
    "        nw_h = n_h // self.window_size ## how many windows we have \n",
    "        nw_w = n_w // self.window_size ## [56 // 7, 28 // 7, 14 // 7, 7 // 7] => [8, 4, 2, 1]\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "        \n",
    "        ### q: [batchsize, (3, 6, 12, 24), (64, 16, 4, 1), 49, 32]\n",
    "        ### k: [batchsize, (3, 6, 12, 24), (64, 16, 4, 1), 49, 32]\n",
    "        ### v: [batchsize, (3, 6, 12, 24), (64, 16, 4, 1), 49, 32]\n",
    "\n",
    "        #### new version on swin v2 ####\n",
    "        self.tau = nn.Parameter(torch.tensor(0.01), requires_grad=True)\n",
    "\n",
    "        q = f.normalize(q, p=2 , dim = -1)\n",
    "        k = f.normalize(k, p=2 , dim = -1)\n",
    "\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau\n",
    "        #### ###################### ###\n",
    "\n",
    "        # uncomment for swin v1\n",
    "        # dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "            ## b: batchsize\n",
    "            ## h: # of heads: (3, 6, 12, 24)\n",
    "            ## w: (64, 16, 4, 1)\n",
    "            ## i: 49 \n",
    "            ## j: 49\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        ## batchsize, h = (3, 6, 12, 24), w= (64, 16, 4, 1), i = 49, j = 49\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        ## batchsize, h = (3, 6, 12, 24), (nw_h * nw_w) = (64, 16, 4, 1), (w_h * w_w) =49, 32)\n",
    "\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        ## [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        ## dim = hidden_dim = (96, 192, 384, 768)\n",
    "        ## head = (3, 6, 12, 24)\n",
    "        ## mlp_dim = hidden_dim * 4\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)  # [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        x = self.mlp_block(x)        # [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_merge = nn.Conv2d(\n",
    "            in_channels  = in_channels,\n",
    "            out_channels = out_channels,\n",
    "            kernel_size  = downscaling_factor,\n",
    "            stride = downscaling_factor,\n",
    "            padding = 0\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        # x shape: (batchsize, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14))\n",
    "        x = self.patch_merge(x)      # [batchsize, (96, 192, 384, 768), (56, 28, 14, 7), (56, 28, 14, 7) ]\n",
    "        return x.permute(0, 2, 3, 1) # [batchsize, (56, 28, 14, 7),     (56, 28, 14, 7), (96, 192, 384, 768)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## x shape: (batchsize, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14))\n",
    "        x = self.patch_partition(x)     # [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)        # [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "            x = shifted_block(x)        # [batchsize, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        return x.permute(0, 3, 1, 2)    # [batchsize, 768 , 7, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        ### image shape (batchsize, 3, 224, 224)\n",
    "        x = self.stage1(img)       ## [batchsize, 96,  56, 56]\n",
    "        x = self.stage2(x)         ## [batchsize, 192, 28 ,28]\n",
    "        x = self.stage3(x)         ## [batchsize, 384, 14, 14]\n",
    "        x = self.stage4(x)         ## [batchsize, 768, 7,  7 ]\n",
    "        x = x.mean(dim=[2, 3])     ## [batchsize, 768]\n",
    "        return self.mlp_head(x)    ## [batchsize, 1000]\n",
    "\n",
    "\n",
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "def swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "def swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "def swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = swin_t(channels = 3, \n",
    "             num_classes = 3, \n",
    "             head_dim = 32, \n",
    "             window_size = 7, \n",
    "             downscaling_factors = (4,2,2,2), \n",
    "             relative_pos_embedding = False)\n",
    "\n",
    "img = torch.randn((1,3,224,224))\n",
    "net(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type (var_name))                                                     Input Shape          Output Shape         Param #              Trainable\n",
       "===========================================================================================================================================================\n",
       "SwinTransformer (SwinTransformer)                                           [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
       "├─StageModule (stage1)                                                      [1, 3, 224, 224]     [1, 96, 56, 56]      --                   Partial\n",
       "│    └─PatchMerging_Conv (patch_partition)                                  [1, 3, 224, 224]     [1, 56, 56, 96]      --                   True\n",
       "│    │    └─Conv2d (patch_merge)                                            [1, 3, 224, 224]     [1, 96, 56, 56]      4,704                True\n",
       "│    └─ModuleList (layers)                                                  --                   --                   --                   Partial\n",
       "│    │    └─ModuleList (0)                                                  --                   --                   232,710              Partial\n",
       "├─StageModule (stage2)                                                      [1, 96, 56, 56]      [1, 192, 28, 28]     --                   Partial\n",
       "│    └─PatchMerging_Conv (patch_partition)                                  [1, 96, 56, 56]      [1, 28, 28, 192]     --                   True\n",
       "│    │    └─Conv2d (patch_merge)                                            [1, 96, 56, 56]      [1, 192, 28, 28]     73,920               True\n",
       "│    └─ModuleList (layers)                                                  --                   --                   --                   Partial\n",
       "│    │    └─ModuleList (0)                                                  --                   --                   898,182              Partial\n",
       "├─StageModule (stage3)                                                      [1, 192, 28, 28]     [1, 384, 14, 14]     --                   Partial\n",
       "│    └─PatchMerging_Conv (patch_partition)                                  [1, 192, 28, 28]     [1, 14, 14, 384]     --                   True\n",
       "│    │    └─Conv2d (patch_merge)                                            [1, 192, 28, 28]     [1, 384, 14, 14]     295,296              True\n",
       "│    └─ModuleList (layers)                                                  --                   --                   --                   Partial\n",
       "│    │    └─ModuleList (0)                                                  --                   --                   3,556,230            Partial\n",
       "│    │    └─ModuleList (1)                                                  --                   --                   3,556,230            Partial\n",
       "│    │    └─ModuleList (2)                                                  --                   --                   3,556,230            Partial\n",
       "├─StageModule (stage4)                                                      [1, 384, 14, 14]     [1, 768, 7, 7]       --                   Partial\n",
       "│    └─PatchMerging_Conv (patch_partition)                                  [1, 384, 14, 14]     [1, 7, 7, 768]       --                   True\n",
       "│    │    └─Conv2d (patch_merge)                                            [1, 384, 14, 14]     [1, 768, 7, 7]       1,180,416            True\n",
       "│    └─ModuleList (layers)                                                  --                   --                   --                   Partial\n",
       "│    │    └─ModuleList (0)                                                  --                   --                   14,180,742           Partial\n",
       "├─Sequential (mlp_head)                                                     [1, 768]             [1, 3]               --                   True\n",
       "│    └─LayerNorm (0)                                                        [1, 768]             [1, 768]             1,536                True\n",
       "│    └─Linear (1)                                                           [1, 768]             [1, 3]               2,307                True\n",
       "===========================================================================================================================================================\n",
       "Total params: 27,538,503\n",
       "Trainable params: 27,509,691\n",
       "Non-trainable params: 28,812\n",
       "Total mult-adds (M): 214.35\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 130.36\n",
       "Params size (MB): 109.92\n",
       "Estimated Total Size (MB): 240.89\n",
       "==========================================================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=net, \n",
    "        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
