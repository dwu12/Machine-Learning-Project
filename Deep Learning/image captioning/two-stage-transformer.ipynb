{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2086,
     "status": "ok",
     "timestamp": 1669093368639,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "9gpmZBMEhW6b",
    "outputId": "08c6050f-dace-4a9d-de88-c413e91669ae"
   },
   "outputs": [],
   "source": [
    "## Mount Google Drive Data (If using Google Colaboratory)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "except:\n",
    "    print(\"Mounting Failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POT2nFJ0RrWX"
   },
   "source": [
    "In this notebook, we utilize transformer structure to create a Image Captioning model with Pretrained Vit and transformer decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pleJ8B7ChU-_"
   },
   "source": [
    "## Image Captioning Model with Pretrained ViT + Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1525,
     "status": "ok",
     "timestamp": 1669093370160,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "yZfxH5MUhU_A",
    "outputId": "740e4bc6-ded0-4a7b-9ba0-e28b966a1825"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywqZmThOhU_B"
   },
   "source": [
    "## Data Load and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1669093370543,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "750cU8MjhU_B",
    "outputId": "254a8653-dfb5-42d3-a106-7fca3ed9a75b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Flickr8k/captions.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1669093370544,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "vRyBL2CrhU_C",
    "outputId": "6463b0b6-5436-45e6-ba66-847d8cf69f70"
   },
   "outputs": [],
   "source": [
    "print('total number of unique image: ', len(df['image'].unique()))\n",
    "print('total number of df: ', len(df))\n",
    "\n",
    "## plan to use 6091 for training, 1000 for val and 1000 for test\n",
    "\n",
    "## [0:6091*5 = 30455 ] traing\n",
    "## [30455:35455,:] val\n",
    "## [35355: , :] test\n",
    "df_train = df.iloc[:30455, :].reset_index()\n",
    "df_val = df.iloc[30455:35455,:].reset_index()\n",
    "df_test= df.iloc[35455:,:].reset_index()\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJaVmwV5hU_C"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens = None, minimum_frequency = 0 , reserved_tokens = None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        if len(tokens) == 0 or isinstance(tokens, list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "            counter = collections.Counter(tokens)\n",
    "        else:\n",
    "            raise Exception('tokens should be a list of list')\n",
    "\n",
    "        self._token_frequence = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.index_to_token = ['<UNK>'] + reserved_tokens \n",
    "        self.token_to_index = {token: index for index , token in enumerate(self.index_to_token)}\n",
    "\n",
    "        for token , frequency in self._token_frequence: \n",
    "            if frequency < minimum_frequency:\n",
    "                # because we have already sorted the dictionary in decreasing order\n",
    "                break\n",
    "\n",
    "            if token not in self.token_to_index:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_index.get(tokens, 0) # 0 is the index for <UNK>\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.index_to_token[indices]\n",
    "\n",
    "        return [self.index_to_token[index] for index in indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIAWxRjATR0n"
   },
   "source": [
    "## Create vocabulary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoI3Pl8khU_C"
   },
   "outputs": [],
   "source": [
    "tokens = [line.lower().split() for line in df.caption.tolist()]\n",
    "reserved_tokens = ['<BOS>','<EOS>','<PAD>']\n",
    "minimum_frequency = 1\n",
    "vocab = Vocab(tokens,minimum_frequency,reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zFlfwgshU_D"
   },
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, image_path, transform = None, df = None, vocab = None):\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        self.img = self.df['image']\n",
    "        self.caption = self.df['caption']\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.img[index]\n",
    "        caption = self.caption[index]\n",
    "\n",
    "        image = Image.open(os.path.join(self.image_path, image)).convert('RGB')\n",
    "\n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        token = caption.lower().split()\n",
    "\n",
    "        #return image, torch.tensor([self.vocab['<BOS>']] + self.vocab[token] + [self.vocab['<EOS>']])\n",
    "        return image, torch.tensor( self.vocab[token] + [self.vocab['<EOS>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRL86lC3hU_D"
   },
   "source": [
    "## Create traindata set, validation dataest and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvKiQZJuhU_D"
   },
   "outputs": [],
   "source": [
    "class Mycollate():\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self,batch):\n",
    "        images = [i[0].unsqueeze(0) for i in batch] #batch is image and caption -> image = batch[0], caption = batch[1]\n",
    "        images = torch.cat(images, dim = 0)\n",
    "\n",
    "        targets = [i[1] for i in batch]\n",
    "        targets = pad_sequence(targets, batch_first = True, padding_value = self.pad_idx)\n",
    "\n",
    "        return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1669093370545,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "XIRdaeWohU_D",
    "outputId": "acd38241-ba86-4713-c5f9-caa056ef1fe5"
   },
   "outputs": [],
   "source": [
    "image_path = '/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Flickr8k/Images'\n",
    "caption_path = \"/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Flickr8k/captions.txt\"\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(232),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # AddGaussianNoise(0,1)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(232),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = Flickr8kDataset(image_path, transform = transform, df = df_train, vocab = vocab)\n",
    "val_dataset = Flickr8kDataset(image_path, transform = transform,  df = df_val, vocab = vocab )\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "\n",
    "pad_idx = vocab['<PAD>']\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "      dataset = train_dataset,\n",
    "      batch_size = 64,\n",
    "      num_workers = 4,\n",
    "      shuffle = True,\n",
    "      pin_memory = True,\n",
    "      collate_fn = Mycollate(pad_idx = pad_idx)\n",
    "  )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "      dataset = val_dataset,\n",
    "      batch_size = 32,\n",
    "      num_workers = 2,\n",
    "      pin_memory = True,\n",
    "      collate_fn = Mycollate(pad_idx = pad_idx)\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1faZ6F0nhU_E"
   },
   "source": [
    "## Model Create CNN and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-HcPh92hU_E"
   },
   "outputs": [],
   "source": [
    "!pip install -q torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "from torchvision.models import vit_b_32, resnet50\n",
    "ViT = vit_b_32(weights = 'IMAGENET1K_V1')\n",
    "resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "#resnet50 = resnet50()\n",
    "\n",
    "for param in ViT.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9xUJyNUM0PZ"
   },
   "outputs": [],
   "source": [
    "class augmentationCNN(nn.Module):\n",
    "    ## since we are using the pretrained resnet image input (batch_size, 3, 224, 224)\n",
    "    ## output size: (hidden_layer = 1, batch_size, hidden_size)\n",
    "    def __init__(self, hidden_size):\n",
    "        super(augmentationCNN, self).__init__()\n",
    "        self.resnet = resnet50 #use pretrained resnet50\n",
    "        self.resnet.Sequential = nn.Identity()\n",
    "        self.resnet.layer2 = nn.Identity()\n",
    "        self.resnet.layer3 = nn.Identity()\n",
    "        self.resnet.layer4 = nn.Identity()\n",
    "        resnet50.fc = nn.Linear(256, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.transform =  transforms.Compose([\n",
    "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.transform(x)\n",
    "        features = self.resnet(x)    \n",
    "        #output = self.dropout(self.relu(features)) #[batchsize, hidden_size]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DERc65XEhU_E"
   },
   "outputs": [],
   "source": [
    "class encoderCNN(nn.Module):\n",
    "    ## since we are using the pretrained resnet image input (batch_size, 3, 224, 224)\n",
    "    ## output size: (hidden_layer = 1, batch_size, hidden_size)\n",
    "    def __init__(self, hidden_size):\n",
    "        super(encoderCNN, self).__init__()\n",
    "        self.model = ViT         \n",
    "        self.model.heads = nn.Sequential(\n",
    "            nn.Linear(in_features=768, out_features = hidden_size, bias = True)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.augmentationCNN = augmentationCNN(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        augmentation = self.augmentationCNN(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dropout(self.relu(x))\n",
    "        x = x + augmentation\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4242,
     "status": "ok",
     "timestamp": 1669093384532,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "13BxsHxGw8eE",
    "outputId": "f8a22322-1c0e-46ae-953b-a11f73f90c4f"
   },
   "outputs": [],
   "source": [
    "summary(model=encoderCNN(256),\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P26Ek714X5y9"
   },
   "outputs": [],
   "source": [
    "#We use Bahdanau (Additive Addention) since it has learned weight. \n",
    "\n",
    "#Key and Value is the encoder output: here we have [1, batchsize, hidden_size]\n",
    "\n",
    "#Query is the last decoder output: [1, batchsize, hidden_size]\n",
    "\n",
    "#Output is [1, batchsize, hidden_size]\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "  def __init__(self, key_dim, query_dim, num_hiddens):\n",
    "    super(AdditiveAttention, self).__init__()\n",
    "    self.W_k = nn.Linear(key_dim, num_hiddens)\n",
    "    self.W_q = nn.Linear(query_dim, num_hiddens)\n",
    "    self.W_v = nn.Linear(num_hiddens, 1)\n",
    "    self.softmax = nn.Softmax(dim = -1)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, query, key, value, mask):\n",
    "    # query : [batchsize, query_len, dim]\n",
    "    # key   : [batchsize, key_len ,  dim]\n",
    "    # value : [batchsize, value_len, dim]\n",
    "\n",
    "    query = self.W_q(query).unsqueeze(2) #[batchsize, query_len, 1,num_hiddens]\n",
    "    key = self.W_k(key).unsqueeze(1)     #[batchsize, 1, key_len,  num_hiddens]\n",
    "    value = self.W_k(value) #[batchsize, value_len, num_hiddens]\n",
    "\n",
    "    output = torch.tanh(query + key)     #[batchsize, query_len, key_len, num_hiddens]\n",
    "\n",
    "    score = self.W_v(output).squeeze(-1) #[batchsize, query_len, key_len]\n",
    "\n",
    "    if mask is not None:\n",
    "      score = score.masked_fill(mask == 0, -1e20)\n",
    "\n",
    "    weight = self.softmax(score) #[batchsize, query_len, key_len] \n",
    "\n",
    "    return torch.bmm(self.dropout(weight), value)  # [1, batch_size, num_hiddens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVKwz-zaefnE"
   },
   "outputs": [],
   "source": [
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#   def __init__(self, dropout):\n",
    "#     super(ScaledDotProductAttention, self).__init__()\n",
    "#     self.dropout = nn.Dropout(dropout)\n",
    "#     self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "#   def forward(self, query , key, value, mask = None):\n",
    "\n",
    "#     # query : [batchsize, query_len, dim]\n",
    "#     # key   : [batchsize, key_len ,  dim]\n",
    "#     # value : [batchsize, value_len, dim]\n",
    "    \n",
    "#     dim = query.shape[2]\n",
    "\n",
    "#     score = torch.bmm(query, key.transpose(1,2)) / np.sqrt(dim) # [batchsize, query_len, key_len]\n",
    "\n",
    "#     if mask is not None:\n",
    "#       score = score.masked_fill(mask == 0, -1e20)\n",
    "\n",
    "#     attention_weight = self.softmax(score)\n",
    "#     return torch.bmm(self.dropout(attention_weight), value) # [batchsize, query_len, dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKizWr-OO_sR"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embed_size, query_size, key_size, value_size, heads, dropout=0.1):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.heads = heads\n",
    "    self.heads_dim = embed_size // heads\n",
    "    self.attention = AdditiveAttention(self.heads_dim, self.heads_dim, self.heads_dim)\n",
    "    self.query = nn.Linear(query_size, embed_size)\n",
    "    self.key = nn.Linear(key_size , embed_size)\n",
    "    self.value = nn.Linear(value_size, embed_size)\n",
    "    self.fc = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, query , key , value, mask):\n",
    "     # query : [batchsize, query_len, dim]\n",
    "     # key   : [batchsize, key_len ,  dim]\n",
    "     # value : [batchsize, value_len, dim]\n",
    "\n",
    "     querys = self.query(query) #[batchsize, query_len, embed_size]\n",
    "     keys = self.key(key)       #[batchsize, key_len,   embed_size]\n",
    "     values = self.value(value) #[batchsize, value_len, embed_size]\n",
    "\n",
    "     querys = querys.reshape(querys.shape[0], querys.shape[1], self.heads, self.heads_dim)\n",
    "     # [batchsize, query_len, self.heads, self.heads_dim]\n",
    "     querys = querys.permute(0,2,1,3) #[batchsize, self.heads, query_len, self.heads_dim]\n",
    "     querys = querys.reshape(-1, querys.shape[2], querys.shape[3]) #[batchsize*self.heads, query_len, self.heads_dim]\n",
    "\n",
    "     keys = keys.reshape(keys.shape[0], keys.shape[1], self.heads, self.heads_dim)\n",
    "     # [batchsize, key_len, self.heads, self.heads_dim]\n",
    "     keys = keys.permute(0,2,1,3) #[batchsize, self.heads, keys_len, self.heads_dim]\n",
    "     keys = keys.reshape(-1, keys.shape[2], keys.shape[3]) #[batchsize*self.heads, key_len, self.heads_dim]\n",
    "\n",
    "     values = values.reshape(values.shape[0], values.shape[1], self.heads, self.heads_dim)\n",
    "     # [batchsize, value_len, self.heads, self.heads_dim]\n",
    "     values = values.permute(0,2,1,3) #[batchsize, self.heads, value_len, self.heads_dim]\n",
    "     values = values.reshape(-1, values.shape[2], values.shape[3]) #[batchsize*self.heads, value_len, self.heads_dim]\n",
    "\n",
    "     output = self.attention(querys, keys, values, mask) \n",
    "     #[batchsize * self.heads, query_len, self.heads_dim]\n",
    "\n",
    "     output = output.reshape(-1, self.heads, output.shape[1], output.shape[2])\n",
    "     #[batchsize, self.heads, query_len, self.heads_dim]\n",
    "\n",
    "     output = output.permute(0, 2, 1, 3)\n",
    "     #[batchsize, query_len, self.heads,  self.heads_dim]\n",
    "\n",
    "     output = output.reshape(output.shape[0],output.shape[1], -1)\n",
    "     #[batchsize, query_len, embed_size]\n",
    "\n",
    "     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NNvXtaXeiKl"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embed_size, query_size, key_size, value_size, heads, dropout=0.1 , dim_feedforward = 2048):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.attention = MultiHeadAttention(embed_size, query_size, key_size, value_size, heads, dropout)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "    self.feed_forward = nn.Sequential(\n",
    "        nn.Linear(embed_size, dim_feedforward), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward , embed_size)\n",
    "    )\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, query, key, value, mask):\n",
    "    # query : [batchsize, query_len, dim]\n",
    "    # key   : [batchsize, key_len ,  dim]\n",
    "    # value : [batchsize, value_len, dim]\n",
    "    attention = self.attention(query, key, value, mask) #[batchsize, query_len, embed_size]\n",
    "    x = self.dropout( self.norm1(attention + query) )   #[batchsize, query_len, embed_size]\n",
    "    forward = self.feed_forward(x)                      #[batchsize, query_len, embed_size]\n",
    "    output = self.dropout(self.norm2(forward + x))      #[batchsize, query_len, embed_size]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smR2eOFaelbL"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self, embed_size, query_size, key_size, value_size, heads, dim_feedforward , dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.attention = MultiHeadAttention(embed_size, query_size, query_size, query_size, heads, dropout)\n",
    "    self.norm = nn.LayerNorm(embed_size)\n",
    "    self.TransformerBlock = TransformerBlock(embed_size, query_size, key_size, value_size, heads, dropout, dim_feedforward)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, query, value, key, trg_mask, src_mask = None):\n",
    "    attention = self.attention(query,query,query, trg_mask)\n",
    "    query = self.dropout(self.norm(attention + query))\n",
    "    output = self.TransformerBlock(query, key, value, src_mask)\n",
    "    return output #[batchsize, query_len, embed_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deEMCxxSU1e7"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdCxQvorengw"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, \n",
    "               vocab_size,\n",
    "               embed_size,\n",
    "               query_size, \n",
    "               key_size, \n",
    "               value_size,\n",
    "               num_layers,\n",
    "               heads,\n",
    "               dim_feedforward,\n",
    "               dropout,\n",
    "               device):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embed_size = query_size\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, query_size)\n",
    "    \n",
    "    self.layers = nn.ModuleList(\n",
    "        [DecoderLayer(embed_size, query_size, key_size, value_size, heads, dim_feedforward, dropout) for _ in range(num_layers)]\n",
    "    )\n",
    "\n",
    "    self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.device = device\n",
    "\n",
    "    self.positional_embedding = nn.Embedding(500 , embed_size)\n",
    "\n",
    "    #self.positional_embedding = PositionalEncoding(query_size, dropout=0.2)\n",
    "\n",
    "  def forward(self, x, enc_out, trg_mask, src_mask = None):\n",
    "    # x shape [batch_size, seq_length]\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    seq_length = x.shape[1]\n",
    "\n",
    "    positional_embedding = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n",
    "    x = self.dropout(( self.embedding(x) * np.sqrt(self.embed_size) + self.positional_embedding(positional_embedding) ) )\n",
    "\n",
    "    # x = self.embedding(x) * np.sqrt(self.embed_size) \n",
    "    # x = self.dropout(self.positional_embedding(x))\n",
    "\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, enc_out, enc_out, trg_mask, src_mask)\n",
    "\n",
    "    out = self.fc_out(x)\n",
    "\n",
    "    return out #[batchsize, seq_length, vocabsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-dy_kLXhU_E"
   },
   "outputs": [],
   "source": [
    "class CNN2Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 src_embed_size = 256,\n",
    "                 trg_embed_size = 256,\n",
    "                 num_layers = 6,\n",
    "                 heads = 8,\n",
    "                 forward_expansion = 2048,\n",
    "                 dropout = 0.5,\n",
    "                 device = None):\n",
    "        super(CNN2Attention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.encoder = encoderCNN(src_embed_size)\n",
    "        self.decoder = Decoder(vocab_size, trg_embed_size, trg_embed_size, src_embed_size, src_embed_size, num_layers, heads, forward_expansion, dropout, device)\n",
    "        #self.augmentation = augmentationCNN(src_embed_size)\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, sequence_length = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((sequence_length, sequence_length))).expand(batch_size * self.heads, sequence_length, sequence_length)\n",
    "        return trg_mask\n",
    "\n",
    "    def make_pad_mask(self,trg):\n",
    "        batch_size, sequence_length = trg.shape\n",
    "\n",
    "        pad_mask = (trg != 3).unsqueeze(2).repeat(self.heads,1,1)\n",
    "        # (batch_size * self.heads, sequence_length, 1)\n",
    "        return pad_mask\n",
    "        \n",
    "    def forward(self, image, caption):\n",
    "        encoder_output = self.encoder(image)\n",
    "        # augmentation_output = self.augmentation(image)\n",
    "\n",
    "        # encoder_output = (encoder_output + augmentation_output) / 2\n",
    "      \n",
    "        bos = torch.tensor([vocab['<BOS>']] * caption.shape[0], device = device).reshape(-1,1)\n",
    "        caption = torch.cat( (bos, caption[:,:-1]), dim = 1)\n",
    "        mask = self.make_trg_mask(caption).to(device)\n",
    "        pad_mask = self.make_pad_mask(caption).to(device)\n",
    "        output = self.decoder(caption, encoder_output, mask, None)\n",
    "        return output #[batchsize, seq_length, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syA7lPLYhU_F"
   },
   "source": [
    "## Model Check Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqnG3s60hU_F"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename = \"/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/cnn_attention_only.pth.tar\"):\n",
    "    print(\"Checkpoint saved\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"Checkpoint loaded\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1669093384534,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "KrK2gHSahU_F",
    "outputId": "c2c44f8e-f5a5-4225-accc-e3911491ce24"
   },
   "outputs": [],
   "source": [
    "load_model = False\n",
    "save_model=False\n",
    "\n",
    "# Model Parameter\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = CNN2Attention( vocab_size = vocab_size, device = device).to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "loss_criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# training epochs\n",
    "num_epochs = 15\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymD6C-ceeKae"
   },
   "outputs": [],
   "source": [
    "def greedy_predict(model, img, vocab, device, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      encoder_output = model.encoder(img) #[batchsize, 1, hiddensize]\n",
    "\n",
    "      target_word_index = [vocab['<BOS>']]\n",
    "      for i in range(max_length):\n",
    "        X = torch.tensor(target_word_index, device = device).unsqueeze(dim = 0)\n",
    "        #trg_mask = model.make_trg_mask(X).cuda()\n",
    "\n",
    "        #encoder_out = encoder_output.expand(encoder_output.shape[0], len(target_word_index), encoder_output.shape[2])\n",
    "        encoder_out = encoder_output \n",
    "        \n",
    "        output = model.decoder(X, encoder_out, None, None)\n",
    "        #print(output.argmax(dim = 2))\n",
    "        # print(vocab.to_tokens(output.argmax(dim=2)[:,-1].item()))\n",
    "\n",
    "        prediction = output.argmax(dim = 2)[:,-1].item() \n",
    "        #target_word_index[i+1] = prediction\n",
    "\n",
    "        if prediction == vocab['<EOS>']:\n",
    "          break\n",
    "\n",
    "        target_word_index.append(prediction)\n",
    "      \n",
    "      return ' '.join(vocab.to_tokens(target_word_index[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUs_zsCtU19f"
   },
   "outputs": [],
   "source": [
    "def beam_search_predict(model, img, vocab, device, max_length = 30, beam = 2, show_step = False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "      k = beam\n",
    "      seqs = torch.LongTensor([[vocab['<BOS>']]] * k).to(device) # [beam_size, 1]\n",
    "      top_k_scores = torch.zeros(k, 1).to(device) # [beam_size, 1]\n",
    "\n",
    "      complete_seqs = []\n",
    "      complete_seqs_scores = []\n",
    "\n",
    "      step = 1\n",
    "\n",
    "      encoder_output = model.encoder(img) #[batchsize = 1, seq_length = 1, hiddensize]\n",
    "      encoder_output = encoder_output.expand(k,encoder_output.shape[1], encoder_output.shape[2]) # [beam_size, 1, hiddensize]\n",
    "\n",
    "      while True:\n",
    "        #encoder_out = encoder_output.expand(encoder_output.shape[0], seqs.shape[1], encoder_output.shape[2]) \n",
    "        encoder_out = encoder_output\n",
    "        output = model.decoder(seqs, encoder_out, None, None) #[beam_size, seq_length, vocabsize]\n",
    "        score = torch.log_softmax(output, dim = 2)\n",
    "        #score = score / ( (seqs.shape[1])**0.5 ) #[beam_size, seq_length, vocabsize], (trick is to make a regularization for large sentence)\n",
    "        \n",
    "        next_token_logits = score[:,-1,:] #(beam_size, vocabsize)\n",
    "\n",
    "        # add \n",
    "        next_token_logits = (top_k_scores.expand_as(next_token_logits) + next_token_logits) \n",
    "  \n",
    "        if step == 1:\n",
    "          top_k_scores, top_k_words = next_token_logits[0].topk(k, dim=0)\n",
    "        else:\n",
    "          top_k_scores, top_k_words = next_token_logits.reshape(-1).topk(k, dim=0) #[beam_size * vocabsize]\n",
    "\n",
    "        #print('step :', step, 'topkscore: ', top_k_scores, 'top_k_words:', top_k_words)\n",
    "\n",
    "        prev_word_index = torch.div(top_k_words, vocab_size, rounding_mode='floor') #[beam_index]\n",
    "        next_word_index = torch.remainder(top_k_words,vocab_size)                   #[token_index]\n",
    "\n",
    "        # print(prev_word_index)\n",
    "        # print(next_word_index)\n",
    "        # prev_word_index : [beam_size]\n",
    "        # next_word_index : [beam_size]\n",
    "        \n",
    "\n",
    "        seqs = torch.cat([seqs[prev_word_index], next_word_index.unsqueeze(1)], dim = 1) #[beam_size, step + 1]\n",
    "        \n",
    "        if show_step:\n",
    "          print(seqs)\n",
    "\n",
    "        incomplete_index = [index for index, next_word in enumerate(next_word_index) if next_word != vocab['<EOS>']]\n",
    "        complete_index = list(set(range(len(next_word_index))) - set(incomplete_index))\n",
    "\n",
    "        # print('is this complete? ', complete_index)\n",
    "\n",
    "        if len(complete_index) > 0:\n",
    "          #print('what is seqs?: ', seqs)\n",
    "          complete_seqs.extend(seqs[complete_index].tolist())\n",
    "          complete_seqs_scores.extend(top_k_scores[complete_index])\n",
    "\n",
    "        k = k - len(complete_index)\n",
    "\n",
    "        if k == 0:\n",
    "          break\n",
    "\n",
    "        seqs = seqs[incomplete_index]\n",
    "        # print('after check complete. what is the seqs?', seqs)\n",
    "        # print(prev_word_index[incomplete_index])\n",
    "\n",
    "        encoder_output = encoder_output[prev_word_index[incomplete_index]]\n",
    "        # print('what is encoder_output shape? ', encoder_output.shape)\n",
    "        \n",
    "        top_k_scores = top_k_scores[incomplete_index].unsqueeze(1) # [some beam_size <= k, 1]\n",
    "        # print('what is top_k_score now?' , top_k_scores)\n",
    "\n",
    "        if step > max_length:\n",
    "          break\n",
    "      \n",
    "        step += 1\n",
    "\n",
    "      if complete_seqs_scores == []:\n",
    "        index = top_k_scores.argmax(dim = 0).item()\n",
    "        l = seqs[index].tolist()[1:]\n",
    "        if l[-1] == vocab['<EOS>']:\n",
    "          l.remove(vocab['<EOS>'])\n",
    "        return ' '.join(vocab.to_tokens(l))\n",
    "\n",
    "      else:\n",
    "        score = max(complete_seqs_scores).item()\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "\n",
    "        if len(complete_index) < beam:\n",
    "          index = top_k_scores.argmax(dim = 0).item()\n",
    "          score2 = top_k_scores.max().item()\n",
    "\n",
    "          if score2 < score:\n",
    "            l = seqs[index].tolist()[1:]\n",
    "            if l[-1] == vocab['<EOS>']:\n",
    "              l.remove(vocab['<EOS>'])\n",
    "            return ' '.join(vocab.to_tokens(l))\n",
    "\n",
    "        seq = complete_seqs[i]\n",
    "        seq = seq[1:]\n",
    "        if seq[-1] == vocab['<EOS>']:\n",
    "          seq.remove(vocab['<EOS>'])\n",
    "        \n",
    "        return  ' '.join(vocab.to_tokens(seq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1669093385688,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "kG_7bOafUe-q",
    "outputId": "8186f888-9562-4757-cd4c-fd046bece0d8"
   },
   "outputs": [],
   "source": [
    "def inference_(df_test):\n",
    "  image_path = '/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Flickr8k/Images'\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(232),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "      # AddGaussianNoise(0,1)\n",
    "  ])\n",
    "\n",
    "  for i in df_test['image'].sample(n=1).tolist():\n",
    "    image = Image.open(os.path.join(image_path, i)).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image2 = image\n",
    "    image2[0] = image2[0] * 0.229\n",
    "    image2[1] = image2[1] * 0.224 \n",
    "    image2[2] = image2[2] * 0.225 \n",
    "    image2[0] += 0.485 \n",
    "    image2[1] += 0.456 \n",
    "    image2[2] += 0.406\n",
    "    plt.imshow(image2.permute(1,2,0))\n",
    "    plt.show()\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "\n",
    "    print('greedy search: ', greedy_predict(model, image, vocab, device, max_length = 20))\n",
    "    print('beam search 1: ', beam_search_predict(model, image, vocab, device, max_length = 20, beam = 1, show_step = False))\n",
    "    print('beam search 3: ', beam_search_predict(model, image, vocab, device, max_length = 20, beam = 3, show_step = False))\n",
    "    print('beam search 5: ', beam_search_predict(model, image, vocab, device, max_length = 20, beam = 5, show_step = False))\n",
    "\n",
    "\n",
    "inference_(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWVTRenbBBJe"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "test_dic = defaultdict(list)\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "  img = df_test.iloc[i,:]['image']\n",
    "  caption = df_test.iloc[i,:]['caption']\n",
    "  test_dic[img].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3187,
     "status": "ok",
     "timestamp": 1669093390350,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "BujSPn3yBBpf",
    "outputId": "e76b3ae6-b944-4356-f3c2-f7b315ff60b1"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "from torchmetrics.functional import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMWFugRn-_-k"
   },
   "outputs": [],
   "source": [
    "def inference(df_test):\n",
    "  image_path = '/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Flickr8k/Images'\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(232),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "      # AddGaussianNoise(0,1)\n",
    "  ])\n",
    "\n",
    "  candidate_corpus_gp = []\n",
    "  candidate_corpus_bm1 = []\n",
    "  candidate_corpus_bm3 = []\n",
    "  candidate_corpus_bm5 = []\n",
    "  references_corpus = []\n",
    "\n",
    "  for i in list(test_dic.keys()):\n",
    "    test = [i.lower() for i in test_dic[i]]\n",
    "    references_corpus.append(test)\n",
    "    image = Image.open(os.path.join(image_path, i)).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "\n",
    "    gp = greedy_predict(model, image, vocab, device, max_length = 20)\n",
    "    bm1 = beam_search_predict(model, image, vocab, device, max_length = 20, beam = 1, show_step = False)\n",
    "    bm3 = beam_search_predict(model, image, vocab, device, max_length = 20, beam = 3, show_step = False)\n",
    "    bm5 = beam_search_predict(model, image, vocab, device, max_length = 20, beam = 5, show_step = False)\n",
    "\n",
    "    candidate_corpus_gp.append(gp)\n",
    "    candidate_corpus_bm1.append(bm1)\n",
    "    candidate_corpus_bm3.append(bm3)\n",
    "    candidate_corpus_bm5.append(bm5)\n",
    "    \n",
    "  print('Greedy ')\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=4))\n",
    "\n",
    "  print('BS 1')\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=4))\n",
    "\n",
    "  print('BS 3')\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=4))\n",
    "\n",
    "  print('BS 5')\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5524818,
     "status": "error",
     "timestamp": 1669098915159,
     "user": {
      "displayName": "DI WU",
      "userId": "00334041348669656891"
     },
     "user_tz": 300
    },
    "id": "kp7m6eY9hU_F",
    "outputId": "f8aebc04-4529-4f25-a127-cf1c19d9f589"
   },
   "outputs": [],
   "source": [
    "best_eval_loss = 10000\n",
    "val_interval = 1\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "model.train() \n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch: ', epoch+1)\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint, filename = '/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/11.22.pth.tar')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for idx, (imgs, captions) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=False):\n",
    "        step += 1\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        output = model(imgs, captions) \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # captions shape: [batch_size ,sequence_length]\n",
    "        # output.shape : [batch_size, sequence_length, vocab_size]\n",
    "        # loss entropy loss:\n",
    "            # input: (N, C)\n",
    "            # output: (N)\n",
    "        loss = loss_criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # CLIPPING Method\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    total_train_loss = total_train_loss / step\n",
    "    train_loss_list.append(total_train_loss)\n",
    "    print('Train loss: {}'.format(total_train_loss))\n",
    "    \n",
    "    \n",
    "    ## for inference\n",
    "    inference_(df_test)\n",
    "\n",
    "\n",
    "    val_total_loss = 0\n",
    "    val_step = 0\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        for idx, (imgs, captions) in tqdm(enumerate(val_dataloader), total=len(val_dataloader), leave=False):\n",
    "          val_step += 1\n",
    "          imgs = imgs.to(device)\n",
    "          captions = captions.to(device)\n",
    "\n",
    "          output = model(imgs, captions) \n",
    "          loss = loss_criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "\n",
    "          val_total_loss += loss.item()        \n",
    "          \n",
    "        val_total_loss = val_total_loss / val_step\n",
    "        val_loss_list.append(val_total_loss)\n",
    "        print('val loss: {}'.format(val_total_loss))\n",
    "\n",
    "      if val_total_loss < best_eval_loss:\n",
    "        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint, filename = '/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Yovitr_best.pth.tar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7prOB7Lec9bO"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Yovitr.txt', 'a') as testwritefile_train:\n",
    "  for index, i in enumerate(train_loss_list):\n",
    "    testwritefile_train.write('step :{}, loss score: {}\\n'.format(index,i))\n",
    "\n",
    "with open('/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Yovitr.txt', 'a') as testwritefile_val:\n",
    "  for index, i in enumerate(val_loss_list):\n",
    "    testwritefile_val.write('step :{}, loss score: {}\\n'.format(index,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mhFSUsEtD0_"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "test_dic = defaultdict(list)\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "  img = df_test.iloc[i,:]['image']\n",
    "  caption = df_test.iloc[i,:]['caption']\n",
    "  test_dic[img].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6vWJDIrtH4E"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "from torchmetrics.functional import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj-HiLp5tJxK"
   },
   "outputs": [],
   "source": [
    "def inference(df_test):\n",
    "  image_path = '/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Flickr8k/Images'\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(232),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "  candidate_corpus_gp = []\n",
    "  candidate_corpus_bm1 = []\n",
    "  candidate_corpus_bm3 = []\n",
    "  candidate_corpus_bm5 = []\n",
    "  references_corpus = []\n",
    "\n",
    "  for i in list(test_dic.keys()):\n",
    "    test = [i.lower() for i in test_dic[i]]\n",
    "    references_corpus.append(test)\n",
    "    image = Image.open(os.path.join(image_path, i)).convert('RGB')\n",
    "    image = transform(image)\n",
    "    # image2 = image\n",
    "    # image2[0] = image2[0] * 0.229\n",
    "    # image2[1] = image2[1] * 0.224 \n",
    "    # image2[2] = image2[2] * 0.225 \n",
    "    # image2[0] += 0.485 \n",
    "    # image2[1] += 0.456 \n",
    "    # image2[2] += 0.406\n",
    "    # plt.imshow(image2.permute(1,2,0))\n",
    "    # plt.show()\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "\n",
    "    gp = greedy_predict(model, image, vocab, device, max_length = 30)\n",
    "    bm1 = beam_search_predict(model, image, vocab, device, max_length = 30, beam = 1, show_step = False)\n",
    "    bm3 = beam_search_predict(model, image, vocab, device, max_length = 30, beam = 3, show_step = False)\n",
    "    bm5 = beam_search_predict(model, image, vocab, device, max_length = 30, beam = 5, show_step = False)\n",
    "\n",
    "    # print('greedy search: ', gp)\n",
    "    # print('beam search 1: ', bm1)\n",
    "    # print('beam search 3: ', bm3)\n",
    "    # print('beam search 5: ', bm5)\n",
    "\n",
    "    candidate_corpus_gp.append(gp)\n",
    "    candidate_corpus_bm1.append(bm1)\n",
    "    candidate_corpus_bm3.append(bm3)\n",
    "    candidate_corpus_bm5.append(bm5)\n",
    "    \n",
    "  print('Greedy ')\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_gp, references_corpus, n_gram=4))\n",
    "\n",
    "  print('BS 1')\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus,n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_bm1, references_corpus, n_gram=4))\n",
    "\n",
    "  print('BS 3')\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_bm3, references_corpus,n_gram=4))\n",
    "\n",
    "  print('BS 5')\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=1))\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=2))\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=3))\n",
    "  print(bleu_score(candidate_corpus_bm5, references_corpus,n_gram=4))\n",
    "\n",
    "    \n",
    "\n",
    "inference(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6b1Yp5jnPMX"
   },
   "outputs": [],
   "source": [
    "model = CNN2Attention( vocab_size = vocab_size, device = device).to(device)\n",
    "model.load_state_dict(torch.load('/content/gdrive/MyDrive/Colab Notebooks/Image Captioning/Yovitr_best_8.pth.tar')['state_dict'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "18Wd7hRaGM_2TVdCVBAe06ibtmHtE7ZCd",
     "timestamp": 1669083649200
    },
    {
     "file_id": "1wzt9LUXPPZI9T9j-c4_VAsrhvK4odRTd",
     "timestamp": 1669070686266
    },
    {
     "file_id": "1zPdb8Oi1PHLbX1ds34TsXA_s8RAwf--P",
     "timestamp": 1668529850729
    },
    {
     "file_id": "11A1ivV3l1-RCc6PHb_IswlBI9DbzSi9M",
     "timestamp": 1668290608057
    },
    {
     "file_id": "16bVkk5Q8hfa_yml3r0SSrGMRRy_AVTSM",
     "timestamp": 1668188835420
    },
    {
     "file_id": "17ZBwqpjSs6Tivq0Ndz9U6ceQDW7bWqeQ",
     "timestamp": 1666904133858
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
